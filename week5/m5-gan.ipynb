{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# One-Dimensional GAN\n## Exercise M5, Visual Recognition, Master in Computer Vision\n### Diego Porres, dporres@cvc.uab.es\nPh.D. Student, ADAS Research Group\n\n# TEAM 6\n\n- Marcos Conde (infomar97@gmail.com)\n- Alex Martin (alexmartin299@gmail.com)\n- Jose Manuel López (joseplcam@gmail.com)\n\n## Solution Exercise 1 \n\n```\nExercise 1  tasks you to find the correct set of parameters, hyperparameters, NN architectures, etc., in order to correctly mimic the real data distribution at the end of training.\n\nTry to minimize the number of epochs needed to train the GAN, but there won’t be a penalty if you use however many you need\n```\n\n**Conclusions**\n\n- The initial setup has as main problem that the discriminator learns much faster than the generator and therefore it requires long training to achieve (not very) competitive results.\n- The losses of generator and discriminator does not converge\n\n**Solutions**\n- Reduce the number of epochs -> we believe for this simple data dsitribution it should run faster.\n- `d_repeats=2` meaning that we train the generator in a proportion 2:1 with respect to the discriminator (i.e. 2 generator steps are 1 discriminator step).\n- we do architecture changes to make the network deeper so it can model complex data distributions \n- we do not provide the ablation, but changing `d_repeats=2` is what the most impact and it is a well-known trick for training GANs. Changing the architecture and making it deeper (i.e. increasing the dense units) while mainiting the intial setup also works, but not as optimal.\n- after 10 epochs, we go back to `d_repeats=1` to avoid over-fitting of the generator = generate data in a specific distribution","metadata":{"id":"a3Oq1gytVV7D"}},{"cell_type":"markdown","source":"## So what's this notebook about?\n\nSadly, **we won't deal with image generation in this notebook**. I know this might come as a dissappointment for some, but it is my belief that it will be far more valuable for you to see how to train a simple GAN that tries to imitate one-dimensional data. While certainly the classical way to learn this subject is to, e.g., generate new [handwritten numbers](http://yann.lecun.com/exdb/mnist/), I've seen too many details being lost or overshadowed by the architecture of the networks themselves instead of concentrating more on the training loop and what your model is actually trying to accomplish.\n\nThus, in the broad scheme of things, the contents of this notebook are the following:\n\n* A quick overview of random numbers (we will explain how this is related)\n* The GAN training algorithm in pseudocode and code\n* Exercises for you to complete and send to me\n* (Appendices) How to sample from the latent space, why it's so hard to train a GAN, and how to evaluate one\n\nAll in all, we will be using `PyTorch`, so this notebook will also aim to help you cement your understanding of this library. Lastly, this notebook is a bit wordy, so use the Table of contents to the left to guide yourself.","metadata":{"id":"Fc_1ZVKArmEQ"}},{"cell_type":"markdown","source":"# Theory - Intuition\n\nWe won't delve deep into the theory of GANs, as you can find this in the relevant papers and the class lectures. Instead, I wish to explain to you an equivalent way of understanding what GANs are actually doing. Note that if you prefer the way they were explained in the lecture, then feel free to skip to the **On To Programming** section ahead.\n\nI hope I don't lose you in the following explanation, as this is the key for this whole shebang, so feel free to ask me any questions if some details lose you or are too murky to traverse. You can  find a bit more theory details in the Appendices below, but I again urge you to refer to the referenced papers or websites.","metadata":{"id":"roCAMaomPoBF"}},{"cell_type":"markdown","source":"## On Random Numbers\n\nLet us begin by asking a simple question: how are random numbers generated in your computer? In other words, [can a computer generate a pure random number?](https://engineering.mit.edu/engage/ask-an-engineer/can-a-computer-generate-a-truly-random-number/) Perhaps a completely unrelated question to ask, but bear with me for a moment, I assure you it will all make sense at the end.\n\nIn essence, computers have trouble generating [pure random numbers](https://www.random.org/). Instead, they have an easier task producing [pseudorandom numbers](https://en.wikipedia.org/wiki/Pseudorandom_number_generator) via a specific algorithm, which are a sequence of numbers that approximate the properties of random numbers but are completely deterministic, so long as you have the seed that generated them. A [**seed**](https://en.wikipedia.org/wiki/Random_seed) is the number that starts the pseudorandom number generator algorithm, and I am sure you have encountered it once before, either in the form [`np.random.seed(seed)`](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.seed.html) or [`random.seed(seed)`](https://docs.python.org/3/library/random.html#random.seed), or in popular games like [Minecraft](https://www.minecraft.net) that require a seed to generate a world.\n\nThus, when we use this seed, we will always generate the same sequence of random numbers thereafter, which is super useful for reproducing results. Let's look at a quick example: ","metadata":{"id":"_-uR-K8Npc-P"}},{"cell_type":"code","source":"import random\n\n# We set the seed:\nrandom.seed(42)\n# Let's print 5 uniformly distributed random numbers in [0, 1]:\nprint([random.uniform(0, 1) for _ in range(5)])","metadata":{"id":"rdPEQBptMKhd","outputId":"faa62aeb-06e4-45af-9954-943f36fd55e7","execution":{"iopub.status.busy":"2022-04-22T21:26:27.744117Z","iopub.execute_input":"2022-04-22T21:26:27.744723Z","iopub.status.idle":"2022-04-22T21:26:27.771274Z","shell.execute_reply.started":"2022-04-22T21:26:27.744618Z","shell.execute_reply":"2022-04-22T21:26:27.770478Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"If we set the same seed again and generate 5 uniformly distributed random numbers in $[0, 1]$, we start to generate the same sequence as before:","metadata":{"id":"xbWu9FjjU0Wi"}},{"cell_type":"code","source":"random.seed(42)\nprint([random.uniform(0, 1) for _ in range(5)])","metadata":{"id":"81XSRwDtUtjX","outputId":"3f38513b-3a1f-4420-e227-24c6d0124c57","execution":{"iopub.status.busy":"2022-04-22T21:26:27.962887Z","iopub.execute_input":"2022-04-22T21:26:27.963111Z","iopub.status.idle":"2022-04-22T21:26:27.967830Z","shell.execute_reply.started":"2022-04-22T21:26:27.963086Z","shell.execute_reply":"2022-04-22T21:26:27.966928Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Of course, if we continue printing more numbers, we will generate new ones that we haven't seen before, unless we set the seed again:","metadata":{"id":"rxxV80LhUw5S"}},{"cell_type":"code","source":"print([random.uniform(0, 1) for _ in range(5)])","metadata":{"id":"HGZ2zchOUuza","outputId":"6512f078-3ba8-4af7-f48f-838518ff637e","execution":{"iopub.status.busy":"2022-04-22T21:27:23.937124Z","iopub.execute_input":"2022-04-22T21:27:23.937403Z","iopub.status.idle":"2022-04-22T21:27:23.942938Z","shell.execute_reply.started":"2022-04-22T21:27:23.937360Z","shell.execute_reply":"2022-04-22T21:27:23.942011Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"[Many programmers have a preferred seed](https://blog.semicolonsoftware.de/the-most-popular-random-seeds/) that they always use for some reason, and as you will confirm in the code, I also have one of my own. Feel free to take a side in this pointless 'war', as we have all done before.\n\n","metadata":{"id":"55opLOCaMLIb"}},{"cell_type":"markdown","source":"## Moving on to more complex distributions\n\nNow, it's 'easy' to generate uniformly distributed random numbers in $[0,1]$ by using [`random.random()`](https://hg.python.org/cpython/file/376c2d81d0e2/Lib/random.py#l356) which in turn uses the [Mersenne Twister](https://en.wikipedia.org/wiki/Mersenne_Twister) as the pseudorandom number generator algorithm. What happens when we wish to generate (pseudo)random numbers with a more complex distribution like, say, the [exponential distribution](https://en.wikipedia.org/wiki/Exponential_distribution)?\n\n![Exponential Distribution](https://upload.wikimedia.org/wikipedia/commons/thumb/0/02/Exponential_probability_density.svg/800px-Exponential_probability_density.svg.png \"Exponential distribution\")\n\nThanks to the [inverse transform method](http://www.columbia.edu/~ks20/4404-Sigman/4404-Notes-ITM.pdf), we can achieve just this! In concrete, we know we can generate random numbers $X$ with Cumulative Distribution Function or [CDF](https://en.wikipedia.org/wiki/Cumulative_distribution_function) $F_X$ via its *inverse CDF* $F_X^{-1}$. The process is as follows:\n\n1. Generate $U\\sim \\mathcal{U}(0,1)$\n2. Obtain $F_X^{-1}$\n3. Compute $X=F_X^{-1}(U)$, and $X$ will have the desired distribution!\n\nThis might not be super clear, so here's an example on how to implement this:\n\n> **Example**: Suppose we wish to generate random numbers $X$ that are exponentially distributed with $\\lambda=0.5$, that is, $X\\sim\\text{Exp}(\\lambda=0.5)$, as the <font color='orange'>orange</font> curve in the figure above. We then get a bunch of numbers $U$ that are uniformly distributed in $[0,1]$, and then pass them through the inverse CDF of the exponential distribution. Luckily, there is a closed form of $F_X^{-1}$, which is:\n\n> $$ X=F_X^{-1}(U) = -\\frac{1}{\\lambda} \\log{(1-U)}\\sim\\text{Exp}(\\lambda) $$","metadata":{"id":"sWyH3vxUM3Hp"}},{"cell_type":"code","source":"import numpy as np\n\n# 1. Get U, a bunch of uniformly distributed numbers in [0, 1]:\nU = np.random.uniform(size=2000)\n\n# 2., 3. Pass through the inverse CDF (which we know beforehand):\nlambda_ = 0.5\nX = - np.log(1 - U) / lambda_ # Note: we can also do: X = -np.log(U) / lambda_ ; why?","metadata":{"id":"tFPSheLcRSc7","execution":{"iopub.status.busy":"2022-04-22T21:27:29.133137Z","iopub.execute_input":"2022-04-22T21:27:29.133670Z","iopub.status.idle":"2022-04-22T21:27:29.139740Z","shell.execute_reply.started":"2022-04-22T21:27:29.133630Z","shell.execute_reply":"2022-04-22T21:27:29.138931Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"$X$ above will be exponentially distributed with $\\lambda=0.5$. To really illustrate this, we can plot their distributions via a simple histogram, first of $U$:","metadata":{"id":"uUfr45NrzSEf"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(6, 4))\nsns.distplot(U, norm_hist=True, kde=True,\n             rug = False, rug_kws={\"alpha\": 0.15})\nplt.title('Uniform Distribution in [0, 1]')\nplt.show()","metadata":{"id":"BqhFl0yMR1T0","outputId":"197fefa2-963f-46c7-de13-564ee4e2c92a","execution":{"iopub.status.busy":"2022-04-22T21:27:39.862209Z","iopub.execute_input":"2022-04-22T21:27:39.863125Z","iopub.status.idle":"2022-04-22T21:27:41.011584Z","shell.execute_reply.started":"2022-04-22T21:27:39.863073Z","shell.execute_reply":"2022-04-22T21:27:41.010873Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"And now of $X$, which has the expected exponential distribution:","metadata":{"id":"pg9wBGPDzJWL"}},{"cell_type":"code","source":"plt.figure(figsize=(6, 4))\nsns.distplot(X, norm_hist=True, kde=True,\n             rug=False, rug_kws={\"alpha\": 0.15})\nplt.title(f'Exponential Distribution with $\\lambda={lambda_}$')\nplt.show()","metadata":{"id":"E4qSDYJCk0Dy","outputId":"b9de3ac4-013f-4932-97f2-c04c69072a81","execution":{"iopub.status.busy":"2022-04-22T21:27:46.019252Z","iopub.execute_input":"2022-04-22T21:27:46.019547Z","iopub.status.idle":"2022-04-22T21:27:46.519539Z","shell.execute_reply.started":"2022-04-22T21:27:46.019516Z","shell.execute_reply":"2022-04-22T21:27:46.518060Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Note that in both plots, the KDE is not really exactly what we expected, since it uses a smoothing function. The histogram does agree with our theoretical result, which is what we should focus on, as the KDE will be better used in the following sections.","metadata":{"id":"dreGmIXVcJdF"}},{"cell_type":"markdown","source":"## Higher Dimensional Random Numbers\n\nSo what is happening, why am I wasting your time talking about random numbers you might ask. Sadly for you, the main reason is because before starting my Ph.D., I actually studied Physics and Applied Mathematics, so I love subjects like this one. \n\nThe second reason is a bit more warranted and  straightforward: we can represent images as a high-dimensional vector. Indeed, remember that each pixel has three channels (if we are working with RGB images that is), so we can see an image of size $224\\times224\\times3$ as being just a *massive* vector. \n\nWith this in mind, we can generate a random image like so then:","metadata":{"id":"js1h0_a0P0i4"}},{"cell_type":"code","source":"random_image = np.random.randint(low=0, high=255, size=(224, 224, 3))\n\nplt.imshow(random_image)\nplt.show()","metadata":{"id":"YepCRbw-dz7l","outputId":"3893f3d7-31a5-41c8-8b3a-67ac224cc460","execution":{"iopub.status.busy":"2022-04-22T21:27:51.333852Z","iopub.execute_input":"2022-04-22T21:27:51.334540Z","iopub.status.idle":"2022-04-22T21:27:51.547424Z","shell.execute_reply.started":"2022-04-22T21:27:51.334389Z","shell.execute_reply":"2022-04-22T21:27:51.545673Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Of course, reality is a bit more nuanced than that. Two things should be noted: ***first***, no matter how many times you run the above code, you won't generate any image that has any sort of significant structure (i.e., you won't be generating a person's face anytime soon). That is simply because the number of images that are gibberish is much, much larger than the number of meaningful images of the desired size $224^2$.\n\nIndeed, how many *unique* images of size $224^2$ are there? Since each pixel has three channels and we continue to assume they are of 24 bits (each channel in the range 0 to 255), then the total number of unique images is:\n\n$$(RGB)^{W\\times H}=(256\\times256\\times256)^{224\\times224} = (2^{24})^{50176} =2^{1204224}$$","metadata":{"id":"jUyFTE3yci_D"}},{"cell_type":"markdown","source":"An astronomically large number that makes the [$1.2$ trillion images taken in 2018](https://theconversation.com/of-the-trillion-photos-taken-in-2018-which-were-the-most-memorable-108815) pale in comparison. Given that we'll still be around the same order of magnitude in the current year (around $2^{40}$), we can see that just the number of possible images of size $224\\times 224$ already overshadows the number of images taken per year. \n\n> As a side note, we should note that while the number of possible images of size $224^2$ is large, it is still a **finite** number nonetheless. To me it sort of begs the question: [is graphic art finite?](https://www.researchgate.net/profile/Kim_Williams10/publication/226211320_From_Tiling_the_Plane_to_Paving_Town_Square/links/0c9605375fd52b78fd000000/From-Tiling-the-Plane-to-Paving-Town-Square.pdf#page=30) Even [music isn't safe of this question either](https://youtu.be/DAcjV60RnRw), so we could honestly just ask if art is finite in and of itself. ","metadata":{"id":"0p2eJGq383X9"}},{"cell_type":"markdown","source":"How is it possible then to take a random vector and produce with it an actual image, let alone an image with interesting features? The implausibility of this task may seem daunting, but there is a way...\n\nThis brings us to our ***second*** point: perhaps trying to generate any meaningful image of size $224^2$ is too broad, what if we make it more specific? More concretely, what if we focus on the images that are solely of _**dogs**_? We could imagine, then, that all these dog images exist in a subspace of the $2^{1204224}$ dimensional space that represents *all* the images of this size. We can use the following figure for guidance:\n\n![Image Manifold](https://user-images.githubusercontent.com/24496178/75723169-0e422d80-5cdc-11ea-88e8-0c0685a07372.png \"Image Manifold\")\n[Image Source](https://www.oreilly.com/library/view/generative-deep-learning/9781492041931/)","metadata":{"id":"tZKQkZQL88ZB"}},{"cell_type":"markdown","source":"We can see that some areas of this high-dimensional space contain images of cars, others images of apples, and so on. The [Manifold Hypothesis](https://arxiv.org/abs/1310.0425) for our case will say as follows: there exists a lower-dimensional manifold that contains all the images of dogs, and this manifold is characterized by a distribution $p_{\\text{dogs}}$. So if we are able to find this distribution, then we can simply sample from it and we'll have a brand new image of a dog!\n\nImagine, if you will, a *closed-form* equation for $p_{\\text{dogs}}$. That is, an equation that perfectly describes the probability that an image $x$ of size $224^2$ is of a **dog**. A nigh impossible task you may say, and you may be right! However, we aren't interested in an ***explicit*** version of this probability distribution, just an ***implicit*** one so that we may sample from it, and we have just described such a mechanism above via the inverse transform method:\n\n1. Generate a sequence of *easy* random numbers $U$, such as Uniformly or Normally distributed\n2. Find $F_{\\text{dogs}}^{-1}$\n3. Get $X=F_{\\text{dogs}}^{-1}(U)$, which should be a sequence of many images of dogs (if training proceeds as intended)!","metadata":{"id":"bOttb-cV9ARs"}},{"cell_type":"markdown","source":"Now we can just focus on one problem: how exactly will we find this inverse CDF, $F_{\\text{dogs}}^{-1}$, in step 2 and then use it in step 3?  Writing such a function explicitly as the example we did before with the Exponential distribution seems, again, impossible. However, remember that neural networks are [universal function approximators](https://en.wikipedia.org/wiki/Universal_approximation_theorem), so they can be used exactly for this purpose!\n\nThus, this is the whole beauty of GANs: the Generator $G$ will act as $F_{\\text{dogs}}^{-1}$, so we need only to feed it random vectors of our chosen *easy* distribution, and the output will be an image that should, hopefully, follow the distribution we wish, in this case of dogs:\n\n$$G(z) = x \\sim p_{\\text{dogs}}$$\n\nIn short, we don't need to know the explicit form of this distribution, as the Generator will do it for us in an implicit way.","metadata":{"id":"Lkh_7oOz9DWq"}},{"cell_type":"markdown","source":"## A Game Theory Perspective\n\nAlright, so how do we actually achieve this tremendous task? We start with a clever trick: from a Machine Learning perspective, we've usually dealt with [discriminative models](https://en.wikipedia.org/wiki/Discriminative_model) by training a Neural Network (Discriminator) in a Supervised Learning way. For example, if we wish to classify some images of handwritten numbers as being <font color='green'>Real</font> or <font color='red'>Fake</font>, we have the classical setting:\n\n![A Discriminative network](https://user-images.githubusercontent.com/24496178/73466532-44f5f280-4382-11ea-9fc7-3dadeacda596.png \"A Discriminative network\")","metadata":{"id":"yeHyqzU6QAWO"}},{"cell_type":"markdown","source":"The trick lies in adding another component into this scheme: another NN, the Generator. Why not let both of these networks compete with one another in a clever way, producing exactly the output that we are looking for?\n\n![Typical GAN Architecture](https://user-images.githubusercontent.com/24496178/73466054-96ea4880-4381-11ea-9898-3e0dcbfaa451.png \"Typical GAN Architecture\")\n[Image Source](https://www.freecodecamp.org/news/an-intuitive-introduction-to-generative-adversarial-networks-gans-7a2264a81394/)\n\nSince we are pitting them against each other, the Generator $G$ will try to fool the Discriminator $D$ by generating *fake* data, whereas $D$ will try to distinguish the *real* from the *fake* data. As such, we will define a [minimax](https://en.wikipedia.org/wiki/Minimax) game or more specifically, a zero-sum game. That is, one player will wish to minimize the objective function, whereas the other player wishes to maximize it. \n\nLet us denote by $p_\\text{data}$ the distribution of our dataset $x$, <font color='green'>$p_g$</font> the distribution of our generated data $G(z)$, and $p_z$ the distribution of our prior *easy* distribution $z$. As such, we will have that:\n\n$$D:x\\to[0,1] \\qquad G:z\\to x \\qquad x\\sim p_\\text{data}, z\\sim p_z$$","metadata":{"id":"v6N1ErICBKFn"}},{"cell_type":"markdown","source":"We use the following objective function explained in the lectures:\n\n$$\nV(D, G)=\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\mathrm{data}}(\\boldsymbol{x})}[\\log D(\\boldsymbol{x})]+\\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}}(\\boldsymbol{z})[\\log (1-D(G(\\boldsymbol{z})))]\n$$\n\nSpecifically, one player will seek to minimize it, while the other will seek to maximize it:\n\n$$\n\\min_G \\max_D V(D, G)= \\min_G \\max_D \\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\mathrm{data}}(\\boldsymbol{x})}[\\log D(\\boldsymbol{x})]+\\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}}(\\boldsymbol{z})[\\log (1-D(G(\\boldsymbol{z})))]\n$$\n\n> This makes sense in the following way: $D$ wants to distinguish between real and fake images, so then $D(x)$ should be close to $1$, whereas $D(G(z))$ should be close to $0$. On the other hand, since $G$ wishes to fool $D$, then it seeks to push $D(G(z))$ close to $1$, and we thus arrive at this minimax setting. Try plotting these functions to verify this conclusion.","metadata":{"id":"PNxZS_TUBOoH"}},{"cell_type":"markdown","source":"Note that our objective is basically a saddle point, so the training will be extremely unstable. Indeed, training a GAN is challenging, and some caveats are discussed in the Appendices below.\n\n![Game theory POV](https://user-images.githubusercontent.com/24496178/75048880-8a14cc80-54c9-11ea-87c1-f6e6abf4b82e.png \"Game theory POV\")\n[Image Source](https://www.iangoodfellow.com/slides/)\n\nThis section is basically a summary of what is seen in the lectures, so consult those if you felt a bit lost here.","metadata":{"id":"Hm0KJctIBRh2"}},{"cell_type":"markdown","source":"### On Training a GAN\n\nNow, to conclude: our Generator will output data with a distribution <font color='green'>$p_{g}$</font>, but we wish this to be close to the real data, $p_\\text{data}$. In the following image, we move to the right as training progresses, with <font color='blue'>$p_{d^*}$</font> being the discriminative distribution along the domain of $x$. **Take special attention, as this is the figure we will try to recreate in this notebook.**\n\n![GAN Training Distribution](https://user-images.githubusercontent.com/24496178/73085503-1635d300-3ecf-11ea-85de-1514d8085c43.png \"GAN Training Distribution\")\n[Image Source](https://arxiv.org/abs/1406.2661)\n\nSo, at the beginning (a), the distribution of the generated and real data is distant, but as training progresses, they will (hopefully) match, and our Discriminator kind of knows how to classify each point. In (b), we train $D$ and it converges to the optimal solution:\n\n$$D^{*}(x)=\\frac{p_\\text{data}(x)}{p_\\text{data}(x)+p_g(x)}$$\n\nWe see that <font color='blue'>$p_{d^{*}}$</font> reflects this. Then, in (c), we train the Generator using the signal from the Discriminator $D^{*}$ and see that the generated distribution gets pushed closer to the real distribution. \n\nWe continue this process until we reach (d), where both distributions match perfectly and, if this is the case, $D^{*}(x)=1/2$ always (as $p_\\text{data} = p_g$). In other words, $D$ won't be able to differentiate the generated from the real data, hence it will always output a probability of $1/2$ of the input being real, i.e. a coin flip!\n\nThis will almost never happen, but one can only dream. Therefore, be attentive whenever the Discriminator consistently outputs $1/2$, as it may have converged. For another more in-depth explanation on the topic, check [Colin Raffel's blog post](https://colinraffel.com/blog/gans-and-divergence-minimization.html).\n\n\n","metadata":{"id":"8471e6qzwTZN"}},{"cell_type":"markdown","source":"# On To Programming\n\nThis notebook will assume you have used [`PyTorch`](https://pytorch.org/) before, or at least you are acquainted with it. Should a refresher be needed, check out this repository: [Grokking `PyTorch`](https://github.com/Kaixhin/grokking-pytorch).\n\nLet's get down to business and load the pertinent packages we'll need from this section onward:","metadata":{"id":"aKXpR-QXpFCF"}},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport shutil\nimport os\n\nimport time\n\nfrom typing import Union, Tuple","metadata":{"id":"743Sy0N4JGWJ","execution":{"iopub.status.busy":"2022-04-22T21:28:13.538105Z","iopub.execute_input":"2022-04-22T21:28:13.538880Z","iopub.status.idle":"2022-04-22T21:28:14.796997Z","shell.execute_reply.started":"2022-04-22T21:28:13.538838Z","shell.execute_reply":"2022-04-22T21:28:14.796276Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"The latter will be used for [type hints](https://www.python.org/dev/peps/pep-0484/), of which some find useful and others won't. I apologize if you find yourself in the latter group. We can get the `PyTorch` version that is installed (and is useful when installing the correct [CUDA toolkit](https://anaconda.org/anaconda/cudatoolkit) version if using a GPU) like so:","metadata":{"id":"A8ZKl0e6S0-X"}},{"cell_type":"code","source":"print(f'PyTorch version: {torch.__version__}')","metadata":{"id":"FzGr8Gc-JfH1","outputId":"269199ab-66fe-432c-d1f3-775b0e79ae0e","execution":{"iopub.status.busy":"2022-04-22T21:28:16.842915Z","iopub.execute_input":"2022-04-22T21:28:16.843667Z","iopub.status.idle":"2022-04-22T21:28:16.848410Z","shell.execute_reply.started":"2022-04-22T21:28:16.843627Z","shell.execute_reply":"2022-04-22T21:28:16.847246Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"The following cell will display a message only if you are using a GPU. In this notebook, a GPU won't be necessary (no *significant* speedup was observed due to the complexity and size of the data), but you are free to use one if you so wish:","metadata":{"id":"aP3CmrsGsutZ"}},{"cell_type":"code","source":"if torch.cuda.is_available():\n    n_gpu = torch.cuda.device_count()\n    for i in range(n_gpu):\n        high, low = torch.cuda.get_device_capability(i)\n        print(f'GPU {i}, Device: {torch.cuda.get_device_name(i)}, Compute Capability: {high}.{low}')\n\n    print(f'Current device: {torch.cuda.current_device()}') # CUDA devices start counting from 0","metadata":{"id":"ZPHIi6jEPidz","execution":{"iopub.status.busy":"2022-04-22T21:28:19.399304Z","iopub.execute_input":"2022-04-22T21:28:19.400072Z","iopub.status.idle":"2022-04-22T21:28:19.456051Z","shell.execute_reply.started":"2022-04-22T21:28:19.399995Z","shell.execute_reply":"2022-04-22T21:28:19.455091Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"You should expect the output to be `0`. In the future, if you are wishing to work with multiple GPUs such as in a server, perhaps [this guide](https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html) will be of your interest.\n\nIn any case, we will be using either `cpu` or `cuda` (GPU), so we can define our `device` like so:","metadata":{"id":"qAQe1GGhtJG4"}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f'Device: {device}')","metadata":{"id":"AqMAaGmq3g0x","outputId":"4f60fc67-f03a-40ae-d528-fcce44a7b601","execution":{"iopub.status.busy":"2022-04-22T21:28:19.968396Z","iopub.execute_input":"2022-04-22T21:28:19.970422Z","iopub.status.idle":"2022-04-22T21:28:19.976175Z","shell.execute_reply.started":"2022-04-22T21:28:19.970388Z","shell.execute_reply":"2022-04-22T21:28:19.975364Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"We will be allocating our models and data to this `device`, so don't forget it!","metadata":{"id":"VgkkSqrHi5gX"}},{"cell_type":"markdown","source":"# The Training Data\n\nLet's get some training data. We can of course get some one-dimensional data from the wild and try to mimic it; however, it's best to be able to easily define the data and know beforehand what to expect in order for the whole GAN training loop to become familiar. \n\nThis is exactly what we will do: our data will be drawn from a Gaussian distribution with mean $\\mu=4.0$ and standard deviation $\\sigma=0.5$. In other words, $p_{\\text{data}}=\\mathcal{N}(4.0, 0.5^2)$. Let's go ahead and create this dataset.","metadata":{"id":"UZwnNYYJR8S3"}},{"cell_type":"markdown","source":"## [Normal Distribution](https://pytorch.org/docs/stable/distributions.html#normal): $p_{\\text{data}}=\\mathcal{N}(\\mu, \\sigma^2)$","metadata":{"id":"TZMdqQfIMPVE"}},{"cell_type":"markdown","source":"The following class definition is not really necessary, but it makes it easier to modify the values of the mean $\\mu$ and standard deviation $\\sigma$ of our data. Likewise, at the end of the notebook you'll have other more complex distributions to choose from, all of which follow this same structure, so this will make it easier to understand those other distributions.","metadata":{"id":"DsAfc9xVjnj2"}},{"cell_type":"code","source":"class NormalDistribution:\n    def __init__(self, mu=4.0, sigma=0.5):\n        self.mu = torch.tensor([mu])\n        self.sigma = torch.tensor([sigma])\n\n    def sample(self, N, seed=42):\n        # Set the seed for reproducibility:\n        torch.manual_seed(seed)\n        # Define the distribution:\n        if N is None:\n            N = 1\n        m = torch.distributions.normal.Normal(loc=self.mu, scale=self.sigma)\n        samples = m.sample([N])\n        return samples","metadata":{"id":"ZQ-xeuSiP3iE","execution":{"iopub.status.busy":"2022-04-22T21:28:25.252823Z","iopub.execute_input":"2022-04-22T21:28:25.253711Z","iopub.status.idle":"2022-04-22T21:28:25.262801Z","shell.execute_reply.started":"2022-04-22T21:28:25.253665Z","shell.execute_reply":"2022-04-22T21:28:25.262076Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Then, we will simply use the `sample` method in order to obtain the correct quantity of data points we wish to use by simply setting `N`. For now, let's use `N=10000`, with the defined mean and variance we had discussed before:","metadata":{"id":"ikL4cxOIANA2"}},{"cell_type":"code","source":"data = NormalDistribution().sample(10000)\n\nprint(data.shape)","metadata":{"id":"ZFJNiJAwJXDj","outputId":"b32032ae-adc1-438c-e8fd-3e79b6ab6996","execution":{"iopub.status.busy":"2022-04-22T21:28:25.483779Z","iopub.execute_input":"2022-04-22T21:28:25.485193Z","iopub.status.idle":"2022-04-22T21:28:25.528915Z","shell.execute_reply.started":"2022-04-22T21:28:25.485149Z","shell.execute_reply":"2022-04-22T21:28:25.528201Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"We now calculate the mean and standard deviation of the `data`. While we know the *real* mean and standard deviation of our data distribution, namely $\\mu=4$ and $\\sigma=0.5$, at the end the Generator will only have access to the `data` we feed it and will try to approximate this sample's parameters. We will then calculate this sample mean and standard deviation and store them in the variables `actual_mean` and `actual_std`, to be used for some plotting later on:","metadata":{"id":"dvviK7hkAVEe"}},{"cell_type":"code","source":"actual_mean = torch.mean(data)\nactual_std = torch.std(data)\n\nprint(f'Data mean: {actual_mean:.4f}, standard deviation: {actual_std:.4f}')","metadata":{"id":"wjHA8i8xLw0n","outputId":"a7e553a2-42bc-4242-90e4-033f1f61ac57","execution":{"iopub.status.busy":"2022-04-22T21:30:02.714413Z","iopub.execute_input":"2022-04-22T21:30:02.714711Z","iopub.status.idle":"2022-04-22T21:30:02.731977Z","shell.execute_reply.started":"2022-04-22T21:30:02.714680Z","shell.execute_reply":"2022-04-22T21:30:02.731100Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"We can see that they don't differ by much from the actual ones, but it's best to keep them stored to better visualize what the Generator is trying to approximate.\n\nNow, let's plot this data: we will use a [kernel density estimate](https://en.wikipedia.org/wiki/Kernel_density_estimation) from [`Seaborn`](https://seaborn.pydata.org/generated/seaborn.kdeplot.html) to better visualize the distribution of the data, along with a histogram in the background. We also plot the individual datapoints in the bottom as a [rugplot](https://seaborn.pydata.org/generated/seaborn.rugplot.html), resulting in:","metadata":{"id":"kdLdPA1Vm7BD"}},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nsns.distplot(data, \n             norm_hist=True,\n             rug=True,\n             kde_kws={\"color\": \"k\", \n                      \"label\": \"Data Distribution\",\n                      \"linestyle\":\":\",\n                      \"linewidth\": 3},\n             rug_kws={\"alpha\": 0.15})\nplt.show()","metadata":{"id":"JF4pvzvzJeYs","outputId":"5a230306-79ae-4fff-9bee-10e732ff5715","execution":{"iopub.status.busy":"2022-04-22T21:30:02.941568Z","iopub.execute_input":"2022-04-22T21:30:02.941877Z","iopub.status.idle":"2022-04-22T21:30:03.421536Z","shell.execute_reply.started":"2022-04-22T21:30:02.941844Z","shell.execute_reply":"2022-04-22T21:30:03.420856Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"The data seems to be exactly what we expected: a normal distribution centered at $4.0$ with variance $\\sigma^2=0.25$. Of course not a perfect Gaussian curve, but close enough.","metadata":{"id":"tK_Lj1QTnvZi"}},{"cell_type":"markdown","source":"## DataLoader\n\nIn general, we won't be using all our training data at the same time, but partition it into **batches**. Remember that it is advisable to have the **batch size** to be a [power of 2](https://datascience.stackexchange.com/a/20193) to fully use the power of your GPU (or, depending on your architecure, even a [power of 8](https://devblogs.nvidia.com/optimizing-gpu-performance-tensor-cores/)). \n\nLet's use `batch_size = 1024`, but this can be easily modified whenever you so wish:","metadata":{"id":"_kpolv4zficY"}},{"cell_type":"code","source":"batch_size = 1024","metadata":{"id":"DeDtEsjyt4H_","execution":{"iopub.status.busy":"2022-04-22T21:30:07.549483Z","iopub.execute_input":"2022-04-22T21:30:07.549805Z","iopub.status.idle":"2022-04-22T21:30:07.554349Z","shell.execute_reply.started":"2022-04-22T21:30:07.549773Z","shell.execute_reply":"2022-04-22T21:30:07.553421Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"> Even though `1024` is a power of 2, I just want to note that generally in Deep Learning, we should have [$2\\leq \\texttt{batch_size}\\leq32$](https://arxiv.org/abs/1804.07612). The good/bad news is we aren't really going to do ***Deep*** Learning in this notebook, plus our data is extremely simple. As such, using a small batch size won't really help us, and I hope you try this later on. On the other hand, it is not unfeasible to go for larger batch sizes than $32$, which is what the authors of [BigGAN](https://arxiv.org/abs/1809.11096) showed in the paper.","metadata":{"id":"GxxBxL-Qt4lO"}},{"cell_type":"markdown","source":"### Data Transformations\n\nWhen you typically load your dataset with `PyTorch`, you should do so with some [transformations](https://pytorch.org/docs/stable/torchvision/transforms.html): `.ToTensor()`, `Normalize()` are perhaps the most important, but of course there are many more. Look into [this guide](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) for writing more custom transformations. \n\nThis is a classical preprocessing step, moreso when your dataset consists of images that you wish to resize, crop, rotate, etc. (typically done so that your model is better able to generalize). Take note that you shouldn't be rotating or randomly cropping for GANs applied to images; it is only common to [mirror-flip at most](https://github.com/NVlabs/stylegan2#training-networks). This is because some datasets only make sense when they're upright (e.g., human faces) or cropped at specific places (such as the center of the image). However, this also depends on your dataset, so don't let me stop you from trying other approaches. This is different than the *dataset augmentations* that have appeared for training GANs with small datasets (such as [StyleGAN2-ADA](https://arxiv.org/abs/2006.06676) or [bCR/zCR](https://arxiv.org/abs/2002.04724)), as these have to be carefully applied so as to not to leak to the Generator, so don't confuse these.\n\nA quick example of using the transformations: the next code block would first transform our image data (as it consists of `NumPy` arrays) to a Tensor, and then normalize it by using a per-channel mean and standard deviation (in the strict definition of the word, what we are doing here is [standardization](https://en.wikipedia.org/wiki/Standard_score), but who am I to rename things at this point?): \n\n```python\ndata_transforms = transforms.Compose([\n                                      transforms.ToTensor(),\n                                      transforms.Normalize(mean=(mean1, mean2, mean3), std=(std1, std2, std3))\n                                      ])\n```\n\nAn interesting discussion on both how to calculate the mean and standard deviation of your dataset can be found [here](https://discuss.pytorch.org/t/about-normalization-using-pre-trained-vgg16-networks/23560). Make sure to read the full thread, as well as the revisions made to the accepted answers.\n\nWe won't be using any transformations, so we can get our [`dataloader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) like so:","metadata":{"id":"UJ3LcecDVUMS"}},{"cell_type":"code","source":"dataloader = torch.utils.data.DataLoader(data, \n                                         batch_size=batch_size, \n                                         shuffle=True) # !!!","metadata":{"id":"2gyi9H9VOKwj","execution":{"iopub.status.busy":"2022-04-22T21:30:08.527933Z","iopub.execute_input":"2022-04-22T21:30:08.528626Z","iopub.status.idle":"2022-04-22T21:30:08.533000Z","shell.execute_reply.started":"2022-04-22T21:30:08.528586Z","shell.execute_reply":"2022-04-22T21:30:08.532186Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"`shuffle=True` is crucial, when we don't wish to somehow leak information to our model, should our dataset be neatly separated in different subdirectories. This is not the case here, but it won't hurt our results.\n\nWe will *iterate* through the `dataloader`, getting a fresh batch of random data of size `batch_size`, up until the last batch which should have a size of `1024*(10000/1024-10000//1024)=784`. In order to get a batch, we can simply create an iterator object via [`iter`](https://docs.python.org/3.8/library/functions.html#iter) and load the `.next()` batch like so:\n\n```python\ndataiter = iter(dataloader)\nx = dataiter.next()\n```","metadata":{"id":"kQ9yFLVF4Em9"}},{"cell_type":"markdown","source":"It is more usual to [`enumerate`](https://docs.python.org/3/library/functions.html#enumerate) the batches, like we do next when plotting them. Note that we can see that the expected Gaussian $\\mathcal{N}(4, 0.25)$ can be somewhat inferred per batch.","metadata":{"id":"9Pncn2HCqrmS"}},{"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nfor i, d in enumerate(dataloader):\n    label = f'Batch {i + 1},  Size {len(d)}'\n    sns.distplot(d, hist=False, kde=True, label=label)\nplt.show()","metadata":{"id":"9-jpNXkCTC6H","outputId":"3a1b2645-ff21-47c1-e2b5-3d4d94a741c5","execution":{"iopub.status.busy":"2022-04-22T21:30:10.656202Z","iopub.execute_input":"2022-04-22T21:30:10.656753Z","iopub.status.idle":"2022-04-22T21:30:11.018766Z","shell.execute_reply.started":"2022-04-22T21:30:10.656715Z","shell.execute_reply":"2022-04-22T21:30:11.018068Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"> A caveat: with images, we will of course use smaller batches, and each of these batches won't necessarily represent the general distribution of our dataset. You can imagine this while picturing our previous dataset of dogs: some batches might be of one specific breed, others might be of mixed breeds, so our statistics may be all over the place!\n\n> We harken back to [BigGAN](https://arxiv.org/abs/1809.11096): the authors noted that large batches helped produce better quality images. Indeed, their batch size was $256$ with $8$ gradient accumulations, translating into a batch size of $2048$. Using 8 [V100](https://www.nvidia.com/en-us/data-center/v100/) GPUs, it takes the model [15 days of training time](https://github.com/ajbrock/BigGAN-PyTorch#how-to-use-this-code) to reach the desired number of iterations. As such, this is why only companies like Google are able to train such a monstruous model.","metadata":{"id":"2Q2o5Z4druMw"}},{"cell_type":"markdown","source":"# Generator\n\nFinally, we will start with our neural network coding. Remember that the input of the Generator will be a simple random vector, so we must first define this vector space, which we will do next:","metadata":{"id":"B9s15wv1R6E4"}},{"cell_type":"markdown","source":"## The latent space $\\mathcal{Z}$\n\nIn the literature, this *easy* vector space from which we take the simple random vectors is called the **latent space**. Historically, the random vectors being used were Uniformly distributed, but nowadays it is common for them to be Normally distributed, that is, $p_z = \\mathcal{N}(0,1)$.\n\nThus, the only thing to define is the dimensionality of this vector space, or $|\\mathcal{Z}|$. Typically, the more complicated the task, the higher this dimension. For example, the [DCGAN](https://arxiv.org/abs/1511.06434) used $|\\mathcal{Z}|=100$, and it set the standard that many GAN architectures used thereafter: in some cases, unnecessary, in others, not complex enough.\n\nFor our case, since we are dealing with simple one-dimensional data, we won't be needing such a high number. Let's stick with $|\\mathcal{Z}|=5$ and see how this works for us:","metadata":{"id":"vMQvS6nNQbZq"}},{"cell_type":"code","source":"latent_dim = 5","metadata":{"id":"JqEJCF2lQdO_","execution":{"iopub.status.busy":"2022-04-22T21:30:18.580849Z","iopub.execute_input":"2022-04-22T21:30:18.581406Z","iopub.status.idle":"2022-04-22T21:30:18.585395Z","shell.execute_reply.started":"2022-04-22T21:30:18.581359Z","shell.execute_reply":"2022-04-22T21:30:18.584716Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Let's define a function that will make it easy for us to produce a random set of latent vectors:","metadata":{"id":"wvYpUwlU5nwS"}},{"cell_type":"code","source":"def get_latents(N: int, latent_dim: int) -> torch.Tensor:\n    size = (N, latent_dim)\n    latents = torch.randn(*size, device=device)\n    return latents","metadata":{"id":"TzCg8NZlJkWs","execution":{"iopub.status.busy":"2022-04-22T21:30:18.786952Z","iopub.execute_input":"2022-04-22T21:30:18.787427Z","iopub.status.idle":"2022-04-22T21:30:18.792074Z","shell.execute_reply.started":"2022-04-22T21:30:18.787398Z","shell.execute_reply":"2022-04-22T21:30:18.791200Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"A classical question to ask yourselves is: how do you know whether or not $G$ is indeed generating better outputs as training progresses? Can we rely solely on the losses of our networks? There is an indirect solution: fix a set of latent vectors, which we will call `fixed_latent` and we'll leave unchanged, and see how the output `G(fixed_latent)` evolves over time:","metadata":{"id":"XInepHHGTZWA"}},{"cell_type":"code","source":"fixed_latent = get_latents(N=batch_size, latent_dim=latent_dim)\nprint(fixed_latent.shape)","metadata":{"id":"6RzSpzdnvdxq","outputId":"568c83da-6c81-4aef-e5c9-612b37dfc36b","execution":{"iopub.status.busy":"2022-04-22T21:30:21.423839Z","iopub.execute_input":"2022-04-22T21:30:21.424121Z","iopub.status.idle":"2022-04-22T21:30:24.128805Z","shell.execute_reply.started":"2022-04-22T21:30:21.424089Z","shell.execute_reply":"2022-04-22T21:30:24.127996Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"We will pass these fixed latent vectors through $G$, (hopefully) producing better and better results as training progresses. \n\nFor generating images, this process will be more intuitive: at the beginning we will see that the Generator produces nonsense, but as training progresses, the generated images will get closer and closer to the ones we wish to generate. For example, in the following GIF, a GAN is trying to generate new MNIST numbers:\n\n![DCGAN MNIST training](https://raw.githubusercontent.com/znxlwm/tensorflow-MNIST-GAN-DCGAN/master/MNIST_DCGAN_results/MNIST_DCGAN_generation_animation.gif \"DCGAN Mnist training\")\n[Image Source](https://github.com/znxlwm/tensorflow-MNIST-GAN-DCGAN)\n\nThis type of visualization only makes sense if we are using the same latent vector for each image, so keep this in mind!\n\nAnother example is the following video I set up for you, of which I explain a bit better in the Appendices. In this video, we see how the image generation evolves for a StyleGAN2:","metadata":{"id":"ame3Vg9ESvpC"}},{"cell_type":"markdown","source":"## The Generator\n\nNow, finally we define our Generator! It will be a simple network, as our real data is also very simple. It will have `1` hidden layer, with `15` hidden neurons, with `1` output (as our real data is one-dimensional):","metadata":{"id":"wL2Qo-mM4IFI"}},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, latent_dim=latent_dim, hidden_dim: int = 15, output_size: int = 1):\n        super(Generator, self).__init__()\n        self.main = nn.Sequential(\n            # Input is the latent vector (ReLU output)\n            nn.Linear(in_features=latent_dim, out_features=hidden_dim),\n            # We use ReLU nonlinearity:\n            nn.ReLU(inplace=True),\n            nn.Linear(in_features=hidden_dim, out_features=hidden_dim*2),\n            nn.ReLU(inplace=True),\n            # Hidden layer (linear output)\n            nn.Linear(in_features=hidden_dim*2, out_features=output_size)\n        )\n\n    def forward(self, x):\n        return self.main(x)","metadata":{"id":"o-exeIVXTcGQ","execution":{"iopub.status.busy":"2022-04-22T21:46:38.249159Z","iopub.execute_input":"2022-04-22T21:46:38.249433Z","iopub.status.idle":"2022-04-22T21:46:38.257252Z","shell.execute_reply.started":"2022-04-22T21:46:38.249403Z","shell.execute_reply":"2022-04-22T21:46:38.255872Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"For our ReLU function, we don't wish to make a copy of the weights, so instead we have modified the values in-place via [`inplace=True`](https://discuss.pytorch.org/t/whats-the-difference-between-nn-relu-and-nn-relu-inplace-true/948). Be careful of not using other in-place operations (such as `out += res` in `forward`), as there will be errors. See [here](https://github.com/pytorch/pytorch/issues/5687) for a discussion on this topic.\n\nLet's get our `generator` and print it:","metadata":{"id":"PbueP4B-7c29"}},{"cell_type":"code","source":"generator = Generator(hidden_dim=32).to(device)\n\nprint(generator)","metadata":{"id":"vAtGtLghTvIn","outputId":"accb10f4-978d-4702-c46a-94394d6d6288","execution":{"iopub.status.busy":"2022-04-22T21:46:38.560396Z","iopub.execute_input":"2022-04-22T21:46:38.561276Z","iopub.status.idle":"2022-04-22T21:46:38.569626Z","shell.execute_reply.started":"2022-04-22T21:46:38.561231Z","shell.execute_reply":"2022-04-22T21:46:38.568864Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"markdown","source":"The parameter values of our model can also be printed like so (though this is not advisable for more complex networks):","metadata":{"id":"vjmmismwuS9O"}},{"cell_type":"code","source":"for i, param in enumerate(generator.parameters()):\n    print(param.size())\n    print(param.data)","metadata":{"id":"8W3dKDzFh9oC","outputId":"86ea4246-d234-4f51-ab34-5a36ee68da56","execution":{"iopub.status.busy":"2022-04-22T21:46:38.700507Z","iopub.execute_input":"2022-04-22T21:46:38.700829Z","iopub.status.idle":"2022-04-22T21:46:38.725417Z","shell.execute_reply.started":"2022-04-22T21:46:38.700797Z","shell.execute_reply":"2022-04-22T21:46:38.724692Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"markdown","source":"Sadly, this won't tell us much about our model, like the `summary` we have in [Keras](https://www.tensorflow.org/api_docs/python/tf/keras/Model#summary). Luckily there is now a solution: use [`torchsummary`](https://github.com/sksq96/pytorch-summary)!\n\nShould this package not be installed, run in an cell the following:\n\n```\n!pip install torchsummary\n```","metadata":{"id":"_HUnKVhdgBtP"}},{"cell_type":"code","source":"!pip -q install torchsummary","metadata":{"execution":{"iopub.status.busy":"2022-04-22T21:46:39.033237Z","iopub.execute_input":"2022-04-22T21:46:39.033806Z","iopub.status.idle":"2022-04-22T21:46:48.708184Z","shell.execute_reply.started":"2022-04-22T21:46:39.033771Z","shell.execute_reply":"2022-04-22T21:46:48.707287Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"from torchsummary import summary","metadata":{"id":"UHADdfiec8oY","execution":{"iopub.status.busy":"2022-04-22T21:46:48.711822Z","iopub.execute_input":"2022-04-22T21:46:48.712074Z","iopub.status.idle":"2022-04-22T21:46:48.716039Z","shell.execute_reply.started":"2022-04-22T21:46:48.712047Z","shell.execute_reply":"2022-04-22T21:46:48.715259Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"summary(model=generator, \n        input_size=(latent_dim, ), \n        batch_size=batch_size)","metadata":{"id":"ERKIwHv8fjER","outputId":"264a53e6-bfe3-43f5-f56e-bada0bc1ff78","execution":{"iopub.status.busy":"2022-04-22T21:46:48.717515Z","iopub.execute_input":"2022-04-22T21:46:48.717745Z","iopub.status.idle":"2022-04-22T21:46:48.732026Z","shell.execute_reply.started":"2022-04-22T21:46:48.717711Z","shell.execute_reply":"2022-04-22T21:46:48.731295Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"markdown","source":"# Discriminator\n\nThe discriminator $D$ will take in a datapoint $x$ (be it an image or in our case a real number) and will output the probability that it's class is $\\text{real}$, or:\n\n$$D(x)=\\mathbb{P}[x | y=\\text{real}]=1-\\mathbb{P}[x|y=\\text{fake}]$$\n\nThus we will use the `sigmoid` activation function as the final layer, with `1` neuron as output. We will give it only `1` hidden layer, but this time with `25` hidden neurons:","metadata":{"id":"TeyYW1PPR9px"}},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, input_size: int = 1, hidden_dim: int = 25):\n        super(Discriminator, self).__init__()\n\n        self.main = nn.Sequential(\n            # Input is one-dimensional\n            nn.Linear(in_features=input_size, out_features=hidden_dim),\n            # We use ReLU nonlinearity\n            nn.ReLU(inplace=True),\n            # Our output is a probability, so we use 1 output neuron with sigmoid:\n            nn.Linear(in_features=hidden_dim, out_features=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.main(x)","metadata":{"id":"bB0WIPkHPNfG","execution":{"iopub.status.busy":"2022-04-22T21:46:48.733897Z","iopub.execute_input":"2022-04-22T21:46:48.734144Z","iopub.status.idle":"2022-04-22T21:46:48.740643Z","shell.execute_reply.started":"2022-04-22T21:46:48.734110Z","shell.execute_reply":"2022-04-22T21:46:48.739909Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"markdown","source":"We do the same as before: we define our `discriminator` and print it, along with the parameter values:","metadata":{"id":"nbPDcsZy8Sj_"}},{"cell_type":"code","source":"discriminator = Discriminator(hidden_dim=64).to(device)\n\nprint(discriminator)","metadata":{"id":"oaYeH6XNWP07","outputId":"f45f3d60-3021-4768-f29b-f722506775ed","execution":{"iopub.status.busy":"2022-04-22T21:46:48.742032Z","iopub.execute_input":"2022-04-22T21:46:48.742463Z","iopub.status.idle":"2022-04-22T21:46:48.751347Z","shell.execute_reply.started":"2022-04-22T21:46:48.742413Z","shell.execute_reply":"2022-04-22T21:46:48.750549Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"for i, param in enumerate(discriminator.parameters()):\n    print(param.size())\n    print(param.data)","metadata":{"id":"AVO9zOnvvfSE","outputId":"8d1977e9-32eb-422f-d5ab-ebacbe63d945","execution":{"iopub.status.busy":"2022-04-22T21:46:48.752826Z","iopub.execute_input":"2022-04-22T21:46:48.753643Z","iopub.status.idle":"2022-04-22T21:46:48.770222Z","shell.execute_reply.started":"2022-04-22T21:46:48.753605Z","shell.execute_reply":"2022-04-22T21:46:48.769533Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":"Again, this doesn't add much insights to this model, so we can print the model:","metadata":{"id":"N93YAeIPx459"}},{"cell_type":"code","source":"summary(model=discriminator, \n        input_size=(1, ), \n        batch_size=batch_size)","metadata":{"id":"5T7w2cStx8Gm","outputId":"906d2c73-94c7-41a1-c3af-7a03ef3a717c","execution":{"iopub.status.busy":"2022-04-22T21:46:48.771424Z","iopub.execute_input":"2022-04-22T21:46:48.771838Z","iopub.status.idle":"2022-04-22T21:46:48.782540Z","shell.execute_reply.started":"2022-04-22T21:46:48.771804Z","shell.execute_reply":"2022-04-22T21:46:48.781801Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"markdown","source":"# Weights initialization\n\nWe could've made the initialization whilst defining each network above, but it is best practice to define a function like we will do now, in order to apply it to any network we wish. For all the weights in both hidden networks, we will use the [Kaiming initialization](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_), except for the bias which we will [zero-initialize](http://cs231n.github.io/neural-networks-2/#init):","metadata":{"id":"2W4DL76GcaTp"}},{"cell_type":"code","source":"def weights_init(module):\n    if isinstance(module, nn.Linear):\n        # Initialize the weights with Kaiming Normal (in-place):\n        nn.init.kaiming_normal_(module.weight)\n        # Initialize the bias with zeros (in-place):\n        module.bias.data.fill_(0.0)","metadata":{"id":"lihDId9EccGx","execution":{"iopub.status.busy":"2022-04-22T21:46:48.783490Z","iopub.execute_input":"2022-04-22T21:46:48.783680Z","iopub.status.idle":"2022-04-22T21:46:48.790141Z","shell.execute_reply.started":"2022-04-22T21:46:48.783656Z","shell.execute_reply":"2022-04-22T21:46:48.789058Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"markdown","source":"So, in order to use this `weights_init` function, we recursively apply it to all modules in each of our models via `model.apply()`. Let's start with the Generator like so:","metadata":{"id":"OYwl37kxZuAI"}},{"cell_type":"code","source":"generator.apply(weights_init)","metadata":{"id":"yoXYY2x4cgsL","outputId":"55b35fe9-47cb-43f1-c511-14ca3aecdfdb","execution":{"iopub.status.busy":"2022-04-22T21:46:48.792145Z","iopub.execute_input":"2022-04-22T21:46:48.792745Z","iopub.status.idle":"2022-04-22T21:46:48.800959Z","shell.execute_reply.started":"2022-04-22T21:46:48.792705Z","shell.execute_reply":"2022-04-22T21:46:48.799892Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"markdown","source":"We can examine the values of our parameters and see that they have been indeed correctly initialized as intended:","metadata":{"id":"wulFjxg4dvC1"}},{"cell_type":"code","source":"for param in generator.parameters():\n    print(param.size())\n    print(param.data)","metadata":{"id":"4DS3lLx3f02C","outputId":"fe69f6fb-856d-421a-fab5-ebcbb5285815","execution":{"iopub.status.busy":"2022-04-22T21:46:48.804771Z","iopub.execute_input":"2022-04-22T21:46:48.805586Z","iopub.status.idle":"2022-04-22T21:46:48.824849Z","shell.execute_reply.started":"2022-04-22T21:46:48.805545Z","shell.execute_reply":"2022-04-22T21:46:48.824081Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"markdown","source":"Likewise for the Discriminator:","metadata":{"id":"VVnxNMI4d00V"}},{"cell_type":"code","source":"discriminator.apply(weights_init)","metadata":{"id":"NJj9QcT1vk5n","outputId":"f29561fa-972a-476d-f9db-251f87395fa7","execution":{"iopub.status.busy":"2022-04-22T21:46:48.826141Z","iopub.execute_input":"2022-04-22T21:46:48.826636Z","iopub.status.idle":"2022-04-22T21:46:48.833419Z","shell.execute_reply.started":"2022-04-22T21:46:48.826594Z","shell.execute_reply":"2022-04-22T21:46:48.832311Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"for param in discriminator.parameters():\n    print(param.size())\n    print(param.data)","metadata":{"id":"1nA5V__fvqLD","outputId":"ab83176b-cb96-4cd8-ee08-d60c05c9eff7","execution":{"iopub.status.busy":"2022-04-22T21:46:48.835001Z","iopub.execute_input":"2022-04-22T21:46:48.835779Z","iopub.status.idle":"2022-04-22T21:46:48.851793Z","shell.execute_reply.started":"2022-04-22T21:46:48.835737Z","shell.execute_reply":"2022-04-22T21:46:48.850939Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"markdown","source":"For a more thorough discussion on weight initialization, check the `PyTorch` forums such as [this one](https://discuss.pytorch.org/t/how-to-fix-define-the-initialization-weights-seed/20156).","metadata":{"id":"qYcHQCzNEQ5H"}},{"cell_type":"markdown","source":"# Criterion\n\nRemember that the signal that the Discriminator $D$ is doing a good job will be given in the form:\n\n$$ \\log{D(x)} + \\log{(1-D(G(z)))}$$\n\nwhich the Discriminator wishes to maximize. On the other hand, the signal that the Generator $G$ is doing a good job wil be given in the form:\n\n$$ \\log \\left( 1-D(G(z)) \\right) $$\n\nwhich the Generator wishes to minimize. The easiest setting is to define the *real* image label as 1 and *fake* image label as 0. ","metadata":{"id":"eZQn5Yp2E16g"}},{"cell_type":"code","source":"real_label = 1.0\nfake_label = 0.0","metadata":{"id":"sdgCzO2BYHAB","execution":{"iopub.status.busy":"2022-04-22T21:46:48.853279Z","iopub.execute_input":"2022-04-22T21:46:48.853556Z","iopub.status.idle":"2022-04-22T21:46:48.857579Z","shell.execute_reply.started":"2022-04-22T21:46:48.853519Z","shell.execute_reply":"2022-04-22T21:46:48.856667Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"markdown","source":"  * ***Note***: You can also try experimenting with `real_label = 0.9`, which is called one-sided label smoothing. Section 4.2 of the [2016 NeuriPS GAN Tutorial](https://arxiv.org/abs/1701.00160) by Goodfellow has a further explanation on this.","metadata":{"id":"eySywC2PiQ0v"}},{"cell_type":"markdown","source":"Therefore, we will use the [binary crossentropy loss function](https://en.wikipedia.org/wiki/Cross_entropy), defined as:\n\n$$J(\\mathbf{w}) = -\\frac{1}{N} \\sum_{n=1}^{N}\\left( y_n \\log \\hat{y}_n + (1-y_n) \\log \\right( 1-\\hat{y}_n \\left)  \\right)$$\n\nIn `PyTorch`, this is simply the [`BCELoss()`](https://pytorch.org/docs/stable/nn.html#bceloss), which will be our criterion for whether either network is doing good. We initialize it like so:","metadata":{"id":"jfZOQlvYYGBp"}},{"cell_type":"code","source":"criterion = nn.BCELoss()","metadata":{"id":"Mtf0PHfsE1r4","execution":{"iopub.status.busy":"2022-04-22T21:46:48.859339Z","iopub.execute_input":"2022-04-22T21:46:48.859931Z","iopub.status.idle":"2022-04-22T21:46:48.865101Z","shell.execute_reply.started":"2022-04-22T21:46:48.859856Z","shell.execute_reply":"2022-04-22T21:46:48.864079Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"markdown","source":"When $y=1$, the Discriminator will know that it's being trained on a real batch of data, whereas when $y=0$, it's being trained on a batch of fake (generated) data. We will see how we can use this setting to use the signal from the Discriminator to train the Generator.","metadata":{"id":"NxYK_fySWJLv"}},{"cell_type":"markdown","source":"# Training Loop\n\nRemember the equation:\n\n$$\n\\min _{G} \\max _{D} V(D, G)=\\min _{G} \\max _{D} \\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text {data }}(\\boldsymbol{x})}[\\log D(\\boldsymbol{x})]+\\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}(\\boldsymbol{z})}[\\log (1-D(G(\\boldsymbol{z})))]\n$$\n\nIn practice, since both the Generator and Discriminator will be neural networks, we can then define this game via their respective parameters/weights $\\theta_g$ and $\\theta_d$ like so:\n\n$$\n\\min _{\\theta_g} \\max _{\\theta_d} V(\\theta_d, \\theta_g)=\\min _{\\theta_g} \\max _{\\theta_d} \\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text {data }}(\\boldsymbol{x})}[\\log D_{\\theta_d}(\\boldsymbol{x})]+\\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}(\\boldsymbol{z})}[\\log (1-D_{\\theta_d}(G_{\\theta_g}(\\boldsymbol{z})))]\n$$\n \nThis translates into the following training loop pseudocode (taken form the [original GAN paper](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)):","metadata":{"id":"bFXfCWlYQMc2"}},{"cell_type":"markdown","source":"---\n### Algorithm 1: GAN training loop pseudocode\n---\n\n **for** number of training iterations **do**\n\n* **for** $k$ steps **do**\n* Sample minibatch of $m$ noise samples $\\{\\boldsymbol{z}^{(1)}, \\dotsc, \\boldsymbol{z}^{(m)}\\}$ from noise prior $p_g(\\boldsymbol{z})$\n* Sample minibatch of $m$ examples $\\{\\boldsymbol{x}^{(1)}, \\dotsc, \\boldsymbol{x}^{(m)}\\}$ from the data generating distribution $p_{\\text{data}}(\\boldsymbol{x})$\n* Update the discriminator by **ascending** its stochastic gradient:\n$$ \\nabla_{\\theta_{d}} \\frac{1}{m} \\sum_{i=1}^{m}\\left[\\log D\\left(\\boldsymbol{x}^{(i)}\\right)+\\log \\left(1-D\\circ G\\left(\\boldsymbol{z}^{(i)}\\right)\\right)\\right] $$\n* **end for**\n    * Sample minibatch of $m$ noise samples $\\{\\boldsymbol{z}^{(1)}, \\dotsc, \\boldsymbol{z}^{(m)}\\}$ from noise prior $p_g(\\boldsymbol{z})$\n    * Update the generator by **descending** its stochastic gradient:\n$$\\nabla_{\\theta_{g}} \\frac{1}{m} \\sum_{i=1}^{m} \\log \\left(1-D\\circ G\\left(\\boldsymbol{z}^{(i)}\\right)\\right)$$\n\n **end for**\n---","metadata":{"id":"whyhr0NjyFWs"}},{"cell_type":"markdown","source":"What are we basically saying here? We will define some `training iterations` or `epochs` where we will loop through the training data and train our network. Our Discriminator $D$ will then be trained $k$ times for every time we train the Generator $G$. When we train one of the networks we keep the other fixed, that is, when we train the Discriminator, we keep the Generator fixed and vice versa. Note that some authors use $k=1$, but others have found that $k>1$ yields better results (see [WGAN](https://arxiv.org/abs/1701.07875)).","metadata":{"id":"xaq_1zLHyOO6"}},{"cell_type":"markdown","source":"## A Heuristic\n\nHowever, there is a caveat noted in the original GAN paper and further discussed in the [NeurIPS 2016 Tutorial on GANs](https://arxiv.org/abs/1701.00160): we can see that at the beginning fo training, the distribution of the generated data will be *very* different to the real data, so the Discriminator will have an easy task as it won't have much problem distinguishing between the two of them. Therefore, $D\\circ G(z)$ will be small, and hence the signal for the Generator won't be enough to update its weights significantly. In other words, the minimax game **saturates** quickly, causing the gradients to **vanish**.\n\nTo fix this, the authors propose the following change:\n\n$$\n\\min _{G} \\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}(\\boldsymbol{z})}[\\log (1-D(G(\\boldsymbol{z})))] = \\max_{G} \\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}(\\boldsymbol{z})}[\\log (D(G(\\boldsymbol{z})))]\n$$\n\nWhat we're looking to do is, instead of minimizing the likelihood of the Discriminator being correct, we will now aim to maximize the likelihood of the Discriminator being wrong. This has the same objective of fooling the Discriminator, but now the bad samples get a greater gradient feedback.\n\nIndeed, we can appreciate this whilst plotting both of the curves for the Generator loss $J^{(G)}$. Note that we are only interested in the <font color='blue'>blue</font> and <font color='green'>green</font> curves, as the former is our previous minimax formulation, while the latter is the heuristic we are applying. When $D\\circ G(z)$ is low, the heuristic will yield larger values, whereas the minimax formulation will have a low value.\n\n![Heuristic](https://user-images.githubusercontent.com/24496178/76852566-eb6f5780-684b-11ea-8b44-69aa8e0dbc31.png \"Heuristic\")\n[Image Source](https://arxiv.org/abs/1701.00160)\n\nWe modify this in the pseudocode:","metadata":{"id":"-FX3zHspRXvs"}},{"cell_type":"markdown","source":"---\n### Algorithm 2: GAN training loop pseudocode with heuristic\n---\n**for** number of training iterations **do**\n\n   * **for** $k$ steps **do**\n        * Sample minibatch of $m$ noise samples $\\{\\boldsymbol{z}^{(1)}, \\dotsc, \\boldsymbol{z}^{(m)}\\}$ from noise prior $p_g(\\boldsymbol{z})$\n        * Sample minibatch of $m$ examples $\\{\\boldsymbol{x}^{(1)}, \\dotsc, \\boldsymbol{x}^{(m)}\\}$ from the data generating distribution $p_{\\text{data}}(\\boldsymbol{x})$\n        * Update the discriminator by **ascending** its stochastic gradient:\n        $$\n        \\nabla_{\\theta_{d}} \\frac{1}{m} \\sum_{i=1}^{m}\\left[\\log D\\left(\\boldsymbol{x}^{(i)}\\right)+\\log \\left(1-D\\circ G\\left(\\boldsymbol{z}^{(i)}\\right)\\right)\\right]\n        $$\n\n* **end for**\n    * Sample minibatch of $m$ noise samples $\\{\\boldsymbol{z}^{(1)}, \\dotsc, \\boldsymbol{z}^{(m)}\\}$ from noise prior $p_g(\\boldsymbol{z})$\n    * Update the generator by **ascending** its stochastic gradient:\n    $$\n    \\nabla_{\\theta_{g}} \\frac{1}{m} \\sum_{i=1}^{m} \\log D\\circ G\\left(\\boldsymbol{z}^{(i)}\\right)\n    $$\n\n **end for**\n---","metadata":{"id":"Ba6L0DvLyQJg"}},{"cell_type":"markdown","source":"> In Either case, please note that we are only using the signal from the Discriminator to train both networks by using the Binary Crossentropy as previously discussed. The change will happen when we train the Generator, as we'll indicate that $y=1$ for the batch of fake data that it generates, but we will see that in the following section.","metadata":{"id":"GTsav0ZhWsCF"}},{"cell_type":"markdown","source":"## Implementing the Training Loop\n\nWe have all the components necessary to train our GAN, including the pseudocode, we just need to translate this into code!\n\nThe core loop was adapted from `PyTorch`'s [DCGAN](https://arxiv.org/abs/1511.06434) [tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html). After you've run and completed this notebook, I hope you can confidently go and test your understanding on said DCGAN tutorial.","metadata":{"id":"4JuhNcesqZYw"}},{"cell_type":"markdown","source":"But first, we will define two helper functions. First, a function taht we will be constantly calling in order to plot the distribution of our generated data, along with the distribution of the real data.\n\nIn short, this function will plot the distribution of the generated `fake_data` along with that of the real `data` every `epoch`. No histogram will be plotted, just the `kde`. All plots will be saved at the `./animation` subdir.","metadata":{"id":"4MOpGsbrrDjg"}},{"cell_type":"code","source":"def plot_distribution(data: torch.Tensor, \n                      fake_data: torch.Tensor, \n                      epoch: int, \n                      hist: bool = False, \n                      kde: bool = True, \n                      figsize: Tuple[int] = (8, 6), \n                      root: Union[str, os.PathLike] = os.path.join(os.getcwd(), 'animation')) -> None:\n    \"\"\"Plot the real and fake data distributions at a specific epoch and save the figures at the `root` directory.\"\"\"\n    fig= plt.figure(figsize=figsize)\n    # Some values for the histogram are selected, should you choose to plot it:\n    sns.distplot(fake_data, hist=hist,  norm_hist=True, bins=50, rug=True, kde=kde,\n                 label='Generated Data Distribution', color=\"g\", rug_kws={\"alpha\": 0.1})\n    # We will compare it to the original data distribution:\n    sns.distplot(data, hist=False,  label='Real Data', \n                 kde_kws={'linestyle':'--', 'color': 'k'})\n    plt.title(f\"Generated Data Distribution - Epoch {epoch}\")\n    # Some of the limits will be dependent on the actual data:\n    plt.ylim((0, 1.5))\n    plt.xlim((actual_mean - 4.0, actual_mean + 4.0)) # pure heuristics on my part\n    # If the save path (root) doesn't exist, create it:\n    if not os.path.exists(root):\n        os.mkdir(root)\n\n    plt.tight_layout()\n    # Save the plot of the distribution at that epoch:\n    save_name = os.path.join(root, f'g_distr_epoch{epoch:03d}.png')\n    plt.savefig(save_name)\n    plt.close(fig)","metadata":{"id":"t5ouadLT_lbX","execution":{"iopub.status.busy":"2022-04-22T21:46:48.867116Z","iopub.execute_input":"2022-04-22T21:46:48.867741Z","iopub.status.idle":"2022-04-22T21:46:48.879163Z","shell.execute_reply.started":"2022-04-22T21:46:48.867680Z","shell.execute_reply":"2022-04-22T21:46:48.878452Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"markdown","source":"During training, we will be calculating the runtime after each batch, so we have to format what [`time.time()`](https://docs.python.org/3/library/time.html#time.time) prints normally. The second helper function `format_time` will then format our runtime to a more human-readable string. Of course this is not necessary, especially since each time you fully train your network will take less than a minute, so \n\nThere is no need to understand the following cell code in depth, just have it clear that it takes a time in seconds such as `80` or `80.0` (an `int` or `float`, hence `Union`) and returns a human-readable string, in this case `1m 20s`. This function is adapted from [here](https://github.com/NVlabs/stylegan2/blob/4874628c7dfffaae01f89558c476842b475f54d5/dnnlib/util.py#L111).","metadata":{"id":"rgZu-eytt3TD"}},{"cell_type":"code","source":"def format_time(seconds: Union[int, float]) -> str:\n    \"\"\"Convert the seconds to human readable string with days, hours, minutes and seconds.\"\"\"\n    s = int(np.rint(seconds))\n    if s < 60:\n        return f'{s}s'\n    elif s < 60 * 60:\n        return f'{s//60}m {s%60}s'\n    elif s < 24 * 60 * 60:\n        return f'{s // (60 * 60)}h {(s // 60) % 60}m {s%60}s'\n    else:\n        return f'{s // (24 * 60 * 60)}d {(s // (60 * 60)) % 24}h {(s // 60) % 60}m'","metadata":{"id":"Ioe7XF7ytnsU","execution":{"iopub.status.busy":"2022-04-22T21:46:49.384626Z","iopub.execute_input":"2022-04-22T21:46:49.384853Z","iopub.status.idle":"2022-04-22T21:46:49.390640Z","shell.execute_reply.started":"2022-04-22T21:46:49.384826Z","shell.execute_reply":"2022-04-22T21:46:49.389800Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"markdown","source":"Ok then, we are done setting up everything, let's code the training loop pseudocode. We will have the following arguments:\n* `num_epochs`: the number of epochs to train our GAN\n* `lr`: the learning rate for both the Generator and Discriminator; you are free to modify this and have a different learning rate for each network\n* `num_eval`: denotes how often we print the message status of our training. Note that this number is dependent on how many minibatches you have (per epoch).\n* `data`, `hist`, `kde`, `root`: to be used as arguments for plotting the distribution with the function `plot_distribution` above\n* `d_repeats`: how many times to train $D$ compared to the times we train $G$, or $k$ in the pseudocode above. Instead, what I did here was that we train $G$ every `d_repeats`, which will yield the results we are seeking.\n\nWe will be translating the Algorithm 2 from pseudocode to code now, so make sure to go back and forth in order to better understand what is happening:","metadata":{"id":"2-GnZXhWaWVF"}},{"cell_type":"code","source":"generator = Generator(hidden_dim=32).to(device)\ngenerator.apply(weights_init)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T21:56:09.948636Z","iopub.execute_input":"2022-04-22T21:56:09.948887Z","iopub.status.idle":"2022-04-22T21:56:09.957119Z","shell.execute_reply.started":"2022-04-22T21:56:09.948859Z","shell.execute_reply":"2022-04-22T21:56:09.956369Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"discriminator = Discriminator(hidden_dim=128).to(device)\ndiscriminator.apply(weights_init)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T21:56:09.973285Z","iopub.execute_input":"2022-04-22T21:56:09.973564Z","iopub.status.idle":"2022-04-22T21:56:09.981083Z","shell.execute_reply.started":"2022-04-22T21:56:09.973536Z","shell.execute_reply":"2022-04-22T21:56:09.980263Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"def train(num_epochs: int = 50,\n          lr: float = 1e-4,\n          num_eval: int = 1,\n          data: torch.Tensor = data,\n          hist: bool = True,\n          kde: bool = False,\n          d_repeats: int = 1,\n          root: Union[str, os.PathLike] = os.path.join(os.getcwd(), 'animation')):\n    # Some sanity check:\n    wow = \"Epochs must be at least 1 and an int!\"\n    assert num_epochs > 0 and isinstance(num_epochs, int), wow\n\n    # We will use Adam for both optimizers with same learning rate:\n    optimizerD = torch.optim.Adam(discriminator.parameters(), lr=lr)\n    optimizerG = torch.optim.Adam(generator.parameters(), lr=lr)\n\n    # We will keep track of the generator and discriminator losses:\n    G_losses = []\n    D_losses = []\n\n    # We will log the generated fake data (with the fixed_latent) in order to \n    # track our progression throughout training (i.e., plot it later on):\n    fake_data = []\n\n    # This will remove the previous run subdir (should this be the second time \n    # you run this code; make sure to save the previous run if you so wish):\n    shutil.rmtree(root, ignore_errors=True)\n\n    # Mark the beginning of training time:\n    start_time = time.time()\n\n    print(\"***** Starting training *****\")\n    for epoch in range(num_epochs):\n        # For each batch in our dataloader:\n        for i, d in enumerate(dataloader):\n            ############################################\n            #                  a.              b.\n            # Update D: max log(D(x)) + log(1 - D(G(z)))\n            ############################################\n            ### a. Train D with a real batch. First zero_grad the optimizer:\n            optimizerD.zero_grad()\n            # Move the batch data to the device:\n            real_b = d.to(device)\n            # We want the batch size to create the labels and latent vectors\n            # later on (remember the last batch won't be of size 1024):\n            b_size = real_b.size(0)\n            # Label of real data:\n            label = torch.full((b_size, ), \n                               real_label, # we fill it with ones\n                               device=device)\n            # Classify the real batch with D:\n            output = discriminator(real_b).view(-1)\n            # Calculate the loss on this real batch:\n            err_D_real = criterion(output, label)\n            # Calculate the gradients for D in backward pass:\n            err_D_real.backward()\n            # We calculate the average classification of the real data to monitor\n            # its progression (should start high, then get lower)\n            D_X = output.mean().item()\n\n            ### b. Train D with a fake batch:\n            # Generate a batch of latent vectors:\n            latents = get_latents(N=b_size, \n                                  latent_dim=latent_dim)\n            # Generate fake data with G:\n            fake_b = generator(latents).to(device)\n            # Let's reuse label (hence in-place fill) by filling with 0's:\n            label.fill_(fake_label)\n            # Classify the fake batch with D:\n            output = discriminator(fake_b.detach()).view(-1)\n            # Calculate D's loss on this fake batch:\n            err_D_fake = criterion(output, label)\n            # Calculate the gradients for D in backward pass:\n            err_D_fake.backward()\n            # We calculate the average classification of the fake data to monitor\n            # its progression:\n            D_G_z = output.mean().item()\n\n            # Add both gradients from the all-real and all-fake batches:\n            err_D = err_D_real + err_D_fake\n            # Once we've accumulated both gradients in the backward pass, we take\n            # a step:\n            optimizerD.step()\n\n            # Now on to train the Generator: remember we are in essence training\n            # D d_repeats every time we train G one time\n            \n            if epoch > 10:\n                d_repeats = 1\n                \n            if i % d_repeats == 0:\n                ##############################\n                #                   c.\n                # Update G: max log(D(G(z)))\n                ##############################\n                ## c. Train G with a fake batch:\n                optimizerG.zero_grad()\n                # This time, for the Generator, we fill the label with 1's:\n                label.fill_(real_label)\n                # We updated D before this, so we make another forward-pass of an \n                # all-fake batch:\n                output = discriminator(fake_b).view(-1)\n                # Calculate G's loss based on this output:\n                err_G = criterion(output, label)\n                # Calculate the gradient for G:\n                err_G.backward()\n                # Update G:\n                optimizerG.step()\n\n            #######################################################################\n            # That's it for Algorithm 2; now on to printing some summary statistics\n            #######################################################################\n\n            # I am an order maniac, so I wish to print the number of necessary\n            # spaces depending on the number of digits for epochs and batches:\n            epoch_digits = int(np.log10(num_epochs)) + 1\n            batch_digits = int(np.log10(len(dataloader))) + 1\n            # Print our training statistics every num_eval:\n            if i % num_eval == 0:\n                # Get the current time:\n                runtime = time.time() - start_time\n                # Our code will run in much less than 1 hour, so we really only\n                # need space for 6 strings for the runtime (hence %-6s):\n                log_console = (\"\\r[%{}d/%{}d][%{}d/%{}d]  Runtime: %-6s  Loss_D: %.4f\"\n                \"  Loss_G: %.4f  D(x): %.4f  D(G(z)): %.4f\").format(*2*[epoch_digits], *2*[batch_digits])\n                # Print it:\n                print(log_console % (epoch, num_epochs, i, len(dataloader), format_time(runtime), \n                                     err_D.item(), err_G.item(), D_X, D_G_z))\n\n        # After every epoch, we save the losses:\n        G_losses.append(err_G.item())\n        D_losses.append(err_D.item())\n\n        # Check how the generator is doing by saving G's output on the fixed_latent\n        with torch.no_grad():\n            fake_X = generator(fixed_latent).detach().cpu()\n        fake_data.append(fake_X)\n        # Let's plot this fake data distribution and save it:\n        plot_distribution(data, \n                          fake_X, \n                          epoch, \n                          hist=hist,\n                          kde=kde,\n                          root=root)\n    print(\"\\n***** Finished training *****\")    \n    \n    # Return both losses for the Generator and Discriminator, as well as the \n    # fake data generated with our fixed_latent\n    return G_losses, D_losses, fake_data\n","metadata":{"id":"fBsv_0RpxuXl","execution":{"iopub.status.busy":"2022-04-22T21:56:10.267768Z","iopub.execute_input":"2022-04-22T21:56:10.268002Z","iopub.status.idle":"2022-04-22T21:56:10.289986Z","shell.execute_reply.started":"2022-04-22T21:56:10.267976Z","shell.execute_reply":"2022-04-22T21:56:10.289350Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"markdown","source":"> We have made it so that the training loop will return the generated data, `fake_data`, but in the grand scheme of things, perhaps it would be best to save this fake data to a file so that then it can be loaded. This won't be necessary for our present case, as the generated data is of small size and can be stored into the variable `fake_data` without having us worry about running out of memory.\n\n> Likewise, it is not unusual to save the models every $n$ batches or iterations in our training loop, but since this problem won't take long to train, this won't be necessary. Keep this in mind for future training loops for more complex models you might have in mind. Or, should the model be simple, then for more complex datasets, as each batch might take too many resources.\n\nWe'll train the for `70` epochs and, according to [the Law](https://twitter.com/karpathy/status/801621764144971776) we'll set `lr=3e-4` (make sure to read the second tweet). We'll also print our statistics every `4` batches, and train our Generator the same number of times that we train our Discriminator, that is, `d_repeats=1`:","metadata":{"id":"IUWTwm5jXgbD"}},{"cell_type":"code","source":"G_losses, D_losses, fake_data = train(num_epochs=50, \n                                      lr=3e-4, \n                                      num_eval=4, \n                                      data=data,\n                                      hist=False,\n                                      kde=True,\n                                      d_repeats=2)","metadata":{"id":"KRhDKQSsWTc-","outputId":"ca1ae14f-cbc3-41ea-c102-42bb2352d68f","execution":{"iopub.status.busy":"2022-04-22T21:56:11.706912Z","iopub.execute_input":"2022-04-22T21:56:11.707496Z","iopub.status.idle":"2022-04-22T21:56:31.537724Z","shell.execute_reply.started":"2022-04-22T21:56:11.707461Z","shell.execute_reply":"2022-04-22T21:56:31.536739Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"markdown","source":"Note that $D(x)$ starts near $1$ and ends up near $1/2$, and that $D(G(z))$ starts a bit higher than $1/2$, then gets lower, then ends up near $1/2$. This is exactly what we wish to accomplish, but have we really achieved $p_g = p_\\text{data}$? We'll soon find out.","metadata":{"id":"4SbijAkQEbZZ"}},{"cell_type":"markdown","source":"## Plotting the Losses\n\nTraining is done, we can now plot the losses of both the Discriminator and the Generator. In order to understand these plots, recall that when we've successfully trained our GAN, $D^{*}(x)=1/2$, for any input $x$. Replacing this into the respective losses for each network, we obtain that the 'optimal' value for the losses will be, respectively for the Discriminator and Generator:\n\n$$ J^{(D^*)} = - (\\log{D^*(x)} + \\log{(1-D^*(G(z)))}) = - (\\log{(1/2)} + \\log{(1-1/2)})=2\\log 2 = \\log 4 $$\n\n$$ J^{(G)} = - \\log{D^*(G(z))} =  - \\log{(1/2)}=\\log 2 $$\n\nLet's plot the losses then and see how they compare to these values:","metadata":{"id":"Rl5ZjyQGEBRT"}},{"cell_type":"code","source":"fig = plt.figure(figsize=(8, 6))\nax = fig.add_axes([0, 0, 1, 1])\nplt.plot(D_losses, label=\"Discriminator loss\")\nplt.plot(G_losses, label=\"Generator loss\")\nax.axhline(np.log(2), c='orange', ls='--', label='$\\log{2}$')\nax.axhline(np.log(4), c='blue', ls='--', label='$\\log{4}$')\nplt.legend()\nplt.xlabel(\"Epoch\")\nplt.title('NEW SETUP')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T21:56:34.567899Z","iopub.execute_input":"2022-04-22T21:56:34.568578Z","iopub.status.idle":"2022-04-22T21:56:34.807631Z","shell.execute_reply.started":"2022-04-22T21:56:34.568541Z","shell.execute_reply":"2022-04-22T21:56:34.806977Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"markdown","source":"### Original setup results","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(8, 6))\nax = fig.add_axes([0, 0, 1, 1])\nplt.plot(D_losses, label=\"Discriminator loss\")\nplt.plot(G_losses, label=\"Generator loss\")\nax.axhline(np.log(2), c='orange', ls='--', label='$\\log{2}$')\nax.axhline(np.log(4), c='blue', ls='--', label='$\\log{4}$')\nplt.legend()\nplt.xlabel(\"Epoch\")\nplt.title('ORIGINAL SETUP')\nplt.show()","metadata":{"id":"0vWkIOOlX37D","outputId":"bb9c0986-1b04-4ab3-bba6-0aa451423466","execution":{"iopub.status.busy":"2022-04-22T21:34:20.626425Z","iopub.execute_input":"2022-04-22T21:34:20.626700Z","iopub.status.idle":"2022-04-22T21:34:20.877553Z","shell.execute_reply.started":"2022-04-22T21:34:20.626668Z","shell.execute_reply":"2022-04-22T21:34:20.876858Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"Apparently, they've stabilized around the desired optimal value! Does this mean that our network has converged to a *meaningful* solution, and that the generated data is indeed similar to the actual data?\n\nShort answer: No. Long answer: ***also No, but in bold***. Let us start by analyzing the `fake_data` that we have stored, by first calculating the mean and standard deviation at each epoch, and then plotting these against the real values we have stored at the beginning of our notebook:","metadata":{"id":"_u_8miPtU7lY"}},{"cell_type":"code","source":"means = [d.mean() for d in fake_data]\nstds = [d.std() for d in fake_data]\n\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_axes([0, 0, 1, 1])\nplt.plot(means, label='Generated mean')\nax.axhline(y=actual_mean, c='blue', ls='--', label=f'Actual mean: {actual_mean:.3f}')\nplt.plot(stds, label='Generated standard dev')\nax.axhline(y=actual_std, c='orange', ls='--', label=f'Actual std dev: {actual_std:.3f}')\nplt.xlabel(\"Epoch\")\nplt.title(f\"Final mean: {means[-1]:.3f}, Final std dev: {stds[-1]:.3f}\")\nplt.legend()\nplt.title('NEW SETUP')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T21:57:25.797587Z","iopub.execute_input":"2022-04-22T21:57:25.797874Z","iopub.status.idle":"2022-04-22T21:57:26.026677Z","shell.execute_reply.started":"2022-04-22T21:57:25.797847Z","shell.execute_reply":"2022-04-22T21:57:26.025867Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"markdown","source":"### Original setup results","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(8, 6))\nax = fig.add_axes([0, 0, 1, 1])\nplt.plot(means, label='Generated mean')\nax.axhline(y=actual_mean, c='blue', ls='--', label=f'Actual mean: {actual_mean:.3f}')\nplt.plot(stds, label='Generated standard dev')\nax.axhline(y=actual_std, c='orange', ls='--', label=f'Actual std dev: {actual_std:.3f}')\nplt.xlabel(\"Epoch\")\nplt.title(f\"Final mean: {means[-1]:.3f}, Final std dev: {stds[-1]:.3f}\")\nplt.legend()\nplt.title('ORIGINAL SETUP')\nplt.show()","metadata":{"id":"tF6INImha4RH","outputId":"a13e9512-6535-459f-8d3b-eece141de0de","execution":{"iopub.status.busy":"2022-04-22T21:38:09.668985Z","iopub.execute_input":"2022-04-22T21:38:09.669271Z","iopub.status.idle":"2022-04-22T21:38:09.913678Z","shell.execute_reply.started":"2022-04-22T21:38:09.669243Z","shell.execute_reply":"2022-04-22T21:38:09.912885Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"We can see that the mean is getting close to the `actual_mean`, but the values of the standard deviation aren't improving; indeed, they seem to be diverging from `actual_std`. In short, we haven't really obtained the actual distribution, as $p_{g}$ is still far away from $p_\\text{data}$, i.e., we've **failed to converge**. More on this can be read in Appendix B, though this effect is more drastic for images.\n\nAs a conclusion, a higher/lower loss is not really correlated with the *quality* of our generated data. This was one of the reasons why [WGAN](https://arxiv.org/abs/1701.07875) was such an important paper, as they showed that their change in the loss function leads to a more intuitive correlation between the loss and image quality. I hope that you have the time to read it, should you have the time of course. Honestly, not many changes are needed in order to turn the GAN we have created above into a WGAN, but we will leave that for another time.","metadata":{"id":"q3Go-BOlardm"}},{"cell_type":"markdown","source":"# Results: Animating the Training\n\nLet's explore the fruits of our hard work in a more fun way. We can of course go to the `Files` on the left and inspect each individual disribution plot, or we can do something more interesting and make an animation with these files.\n\nWe will use [`ffmpeg`](https://www.ffmpeg.org/) to grab all the training images in `./animation` in order to make a video of the training process and save it as `./training_video.mp4`. Perhaps the only parameter you can change is `-framerate 20`, as this will control the fps of the training video, so I leave this up to you:","metadata":{"id":"Rw6Zc0Se-lCJ"}},{"cell_type":"code","source":"!ffmpeg -loglevel quiet -y -framerate 20 -i ./animation/g_distr_epoch%3d.png -c:v libx264 -profile:v high -crf 20 -pix_fmt yuv420p training_video.mp4","metadata":{"id":"EWJH-Sodnd-2","execution":{"iopub.status.busy":"2022-04-22T21:57:32.857170Z","iopub.execute_input":"2022-04-22T21:57:32.857717Z","iopub.status.idle":"2022-04-22T21:57:34.046170Z","shell.execute_reply.started":"2022-04-22T21:57:32.857678Z","shell.execute_reply":"2022-04-22T21:57:34.045196Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"markdown","source":"`-loglevel quiet` will suppress the output, which can be quite wordy and uninteresting at times. However, don't use it when starting new projects, as the error outputs will not be uncommon. \n\nThe `-y` flag at the beginning will simply overwrite any other file with the same name, which is useful if you run many times the above code when testing different parameters, hyperparameters, architectures, etc. Another option is for you to simply give different names to each training, but to each their own. \n\nTo obtain a GIF, we can simply use this generated MP4 file:","metadata":{"id":"QvcKhZXzohp6"}},{"cell_type":"code","source":"!ffmpeg -loglevel quiet -y -i training_video.mp4 training_gif.gif","metadata":{"id":"Z7JQQT9iohYt","execution":{"iopub.status.busy":"2022-04-22T21:57:34.048202Z","iopub.execute_input":"2022-04-22T21:57:34.048444Z","iopub.status.idle":"2022-04-22T21:57:35.016310Z","shell.execute_reply.started":"2022-04-22T21:57:34.048415Z","shell.execute_reply":"2022-04-22T21:57:35.015394Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"markdown","source":"If you do not wish to meddle too much with `ffmpeg` and prefer to remain in `Python`, you could also try installing [`ffmpeg-python`'](https://github.com/kkroening/ffmpeg-python) via `!pip install ffmpeg-python`. For example, to recreate the previous cell, you could change it to:\n\n```py\nimport ffmpeg\nffmpeg.input('training_video.mp4').output('training_gif.gif').run(capture_stdout=True, capture_stderr=True)\n```\n\nIn either case, we can then download the generated video or GIF to our local machine to watch it, or simply display it in the notebook like so:","metadata":{"id":"f8YinMSXzHL0"}},{"cell_type":"code","source":"from IPython.display import display, Image\n\nwith open('./training_gif.gif', 'rb') as f:\n    display(Image(data=f.read(), format='png'))","metadata":{"id":"IsuZ4MJaxRFG","outputId":"9ebfa698-6918-4c63-81bb-a5f4843bb958","execution":{"iopub.status.busy":"2022-04-22T21:57:35.018187Z","iopub.execute_input":"2022-04-22T21:57:35.018510Z","iopub.status.idle":"2022-04-22T21:57:35.032167Z","shell.execute_reply.started":"2022-04-22T21:57:35.018469Z","shell.execute_reply":"2022-04-22T21:57:35.031422Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"markdown","source":"We can see in a more visual way what we already plotted and discussed above: the mean of the generated data is getting closer to the real one, but the standard deviation is actually increasing. Bad news! We need better hyperparameters in order to successfully mimic the real data.","metadata":{"id":"9pAs0we5_Pgu"}},{"cell_type":"markdown","source":"> Now, you can find in the `Files` menu to the left the relevant images, video and GIF. Save whichever you wish to save to your local machine, or upload to your Google Drive via running in a cell:\n ```python\nfrom google.colab import drive\ndrive.mount('/content/gdrive/')\n```\nor simply click the button **Mount Drive** on the top left. For more steps regarding mounting Google Drive on Colab, follow [this guide](https://medium.com/@ml_kid/how-to-save-our-model-to-google-drive-and-reuse-it-2c1028058cb2) ","metadata":{"id":"SluBEA3znYkj"}},{"cell_type":"markdown","source":"## Saving and Loading the Models\n\nOf course, assuming we are satisfied with our training, we can just save our models like so:","metadata":{"id":"4A-wcJ45av4o"}},{"cell_type":"code","source":"torch.save(generator.state_dict(), './trained_g.pth')\ntorch.save(discriminator.state_dict(), './trained_d.pth')","metadata":{"id":"xleyTzH2aWyF","execution":{"iopub.status.busy":"2022-04-22T21:57:37.274615Z","iopub.execute_input":"2022-04-22T21:57:37.274925Z","iopub.status.idle":"2022-04-22T21:57:37.284289Z","shell.execute_reply.started":"2022-04-22T21:57:37.274889Z","shell.execute_reply":"2022-04-22T21:57:37.283441Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"markdown","source":"It is best to save the model's `state_dict()` instead of the entire model, as per `PyTorch`'s [best practices](https://pytorch.org/docs/stable/notes/serialization.html). \n\nTo resume from this checkpoint in order to continue training or generate new values, we can load each model like so (note that `hidden_dim` must match to the one you had in your saved `.pth` file).","metadata":{"id":"5TGF9PaVd-nf"}},{"cell_type":"code","source":"g = Generator(hidden_dim=32)\ng.load_state_dict(torch.load('./trained_g.pth'))","metadata":{"id":"WfByxdYPa0or","outputId":"31de86c0-6f5e-4ff4-945b-7023d1e3717e","execution":{"iopub.status.busy":"2022-04-22T21:57:48.604721Z","iopub.execute_input":"2022-04-22T21:57:48.605260Z","iopub.status.idle":"2022-04-22T21:57:48.615909Z","shell.execute_reply.started":"2022-04-22T21:57:48.605220Z","shell.execute_reply":"2022-04-22T21:57:48.615072Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"d = Discriminator(hidden_dim=128)\nd.load_state_dict(torch.load('./trained_d.pth'))","metadata":{"id":"Wg9fnLaJb2fa","outputId":"c8572140-839e-441c-edc3-0b6b39001f9d","execution":{"iopub.status.busy":"2022-04-22T21:57:56.424249Z","iopub.execute_input":"2022-04-22T21:57:56.424515Z","iopub.status.idle":"2022-04-22T21:57:56.432944Z","shell.execute_reply.started":"2022-04-22T21:57:56.424485Z","shell.execute_reply":"2022-04-22T21:57:56.432284Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"markdown","source":"For a more detailed review on this subject, [follow this tutorial](https://pytorch.org/tutorials/beginner/saving_loading_models.html).","metadata":{"id":"TkL9_NFJcd4_"}},{"cell_type":"markdown","source":"# **Tasks**\n\n***Work in groups of up to 3 students***\n***Deadline: April 22nd at midnight.***\n\nFor the following, **make sure you are starting with an untrained Generator and Discriminator**. One way to do this is to run all the cells from the top or, when in doubt, restart the kernel altogether. Each exercise is worth 0.5 points.\n\n1. Clearly, our training has failed to replicate $p_\\text{data}$. What changes in parameters and hyperparameters will make it easier for our network to approximate $p_\\text{data}$? \n\n In your report, I expect you try variations in at least the following parameters/hyperparameters:\n * The number of epochs\n * The latent space dimension $|\\mathcal{Z}|$. Above, we used 5-dim, perhaps a lower or higher value will be more useful.\n    > *Note*: [StyleGAN](https://github.com/NVlabs/stylegan)/[StyleGAN2](https://github.com/NVlabs/stylegan2)/[StyleGAN3](https://github.com/NVlabs/stylegan3) have latent spaces with size $|\\mathcal{Z}|=512$ (plus an intermediate latent space $\\mathcal{W}$, but this is beyond the point); can you infer why? On another direction: do you think sizes these large are necessary?\n * More/less hidden neurons for the Generator and Discriminator. \n    > *Clue*: Do you think it's benefitial for the generator $G$ if the discriminator $D$ is always *stronger* (i.e., more hidden layers, more neurons per layer, more complex architecture, etc.) when compared to $G$? What do your experiments tell you?\n * How many times you train the Discriminator vs how many times you train the Generator per epoch: `d_repeats`.\n * A higher/lower learning rate, perhaps even different learning rates for each netowrk\n * A smaller/larger batch size\n * Try a different weight initialization (or none at all). You can find the default initialization for the layers in `PyTorch` [here](https://discuss.pytorch.org/t/whats-the-default-initialization-methods-for-layers/3157).\n\n Report all trials you perform in the form of graphs, training GIF, final plot of the distribution, however you so choose.\n2. Choose one of the following *harder* distributions for $p_\\text{data}$ (your training dataset) and find the necessary parameters and hyperparameters in order to mimick it, the same as above. Please do so starting from the `\"\"\" TODO...\"\"\"` cell below. \n \n You don't need to report every iteration that you do, just the final set of parameters and hyperparameters that allowed you to correctly imitate the selected data distribution. Reporting the training GIF of this set of parameters, hyperparameters, network architecture, etc. is sufficient here.\n\n     > *Clue*: For some of these distributions, there is a **theoretical** value for the mean and standard deviation. You can calculate these values and use them in your trials in order to ensure you are indeed converging to the desired distribution.","metadata":{"id":"RvE1bW_Hbyvv"}},{"cell_type":"markdown","source":"### [Laplace Distribution](https://en.wikipedia.org/wiki/Laplace_distribution) $p_{\\text{data}}=\\text{Laplace}(\\mu, b)$","metadata":{"id":"CJyvRBOdMH4o"}},{"cell_type":"code","source":"class LaplaceDistribution:\n    def __init__(self, loc=3.0, scale=0.3):\n        self.loc = torch.tensor([loc])\n        self.scale = torch.tensor([scale])\n\n    def sample(self, N, seed=42):\n        # Set the seed for reproducibility:\n        torch.manual_seed(seed)\n        m = torch.distributions.Laplace(loc=self.loc, scale=self.scale)\n        samples = m.sample([N])\n        return samples","metadata":{"id":"lSZeqVuxGcnc","execution":{"iopub.status.busy":"2022-04-22T21:57:58.986724Z","iopub.execute_input":"2022-04-22T21:57:58.986988Z","iopub.status.idle":"2022-04-22T21:57:58.994614Z","shell.execute_reply.started":"2022-04-22T21:57:58.986957Z","shell.execute_reply":"2022-04-22T21:57:58.993916Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"code","source":"data = LaplaceDistribution(loc=3.0, scale=0.3).sample(10000)","metadata":{"id":"Appqt7exG9Yx","execution":{"iopub.status.busy":"2022-04-22T21:58:09.244535Z","iopub.execute_input":"2022-04-22T21:58:09.244821Z","iopub.status.idle":"2022-04-22T21:58:09.282126Z","shell.execute_reply.started":"2022-04-22T21:58:09.244793Z","shell.execute_reply":"2022-04-22T21:58:09.281400Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nsns.distplot(data, kde=False, rug=True, bins=150,\n             norm_hist=True, hist_kws={'color': 'red'},\n             rug_kws={'alpha': 0.05, 'color': 'navy'})\nplt.show()","metadata":{"id":"pREtYxSCHLH3","outputId":"60c1b270-b20a-4d58-9abf-440f2c23d5f6","execution":{"iopub.status.busy":"2022-04-22T21:58:09.369177Z","iopub.execute_input":"2022-04-22T21:58:09.369381Z","iopub.status.idle":"2022-04-22T21:58:09.909608Z","shell.execute_reply.started":"2022-04-22T21:58:09.369356Z","shell.execute_reply":"2022-04-22T21:58:09.908871Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"markdown","source":"### [Half Normal Distribution](https://en.wikipedia.org/wiki/Half-normal_distribution)","metadata":{"id":"X2cLMkUZ-qM4"}},{"cell_type":"code","source":"class HalfNormalDistribution:\n    def __init__(self, scale=0.75):\n        self.scale = torch.tensor([scale])\n\n    def sample(self, N, seed=42):\n        # Set the seed for reproducibility:\n        torch.manual_seed(seed)\n        m = torch.distributions.HalfNormal(scale=self.scale)\n        samples = m.sample([N])\n\n        return samples","metadata":{"id":"stOLYrbeAbhy","execution":{"iopub.status.busy":"2022-04-22T21:58:10.486780Z","iopub.execute_input":"2022-04-22T21:58:10.487414Z","iopub.status.idle":"2022-04-22T21:58:10.492725Z","shell.execute_reply.started":"2022-04-22T21:58:10.487377Z","shell.execute_reply":"2022-04-22T21:58:10.491931Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"data = HalfNormalDistribution(scale=0.75).sample(10000)","metadata":{"id":"QZ9nAqzxAfqe","execution":{"iopub.status.busy":"2022-04-22T21:58:11.368469Z","iopub.execute_input":"2022-04-22T21:58:11.368823Z","iopub.status.idle":"2022-04-22T21:58:11.374058Z","shell.execute_reply.started":"2022-04-22T21:58:11.368790Z","shell.execute_reply":"2022-04-22T21:58:11.373182Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nsns.distplot(data, kde=False, rug=True, bins=100,\n             norm_hist=True, hist_kws={'color': 'darkgreen'},\n             rug_kws={'alpha': 0.05, 'color': 'purple'})\nplt.show()","metadata":{"id":"AxchDcBDAhvk","outputId":"f9831be3-40ef-4efd-d743-432f29106a9f","execution":{"iopub.status.busy":"2022-04-22T21:58:12.303504Z","iopub.execute_input":"2022-04-22T21:58:12.303754Z","iopub.status.idle":"2022-04-22T21:58:12.755581Z","shell.execute_reply.started":"2022-04-22T21:58:12.303722Z","shell.execute_reply":"2022-04-22T21:58:12.754840Z"},"trusted":true},"execution_count":142,"outputs":[]},{"cell_type":"markdown","source":"### [Petit Prince Distribution](https://66.media.tumblr.com/tumblr_lzf2o3epcz1qkww7to1_400.jpg)","metadata":{"id":"CHjcwblPrGqS"}},{"cell_type":"code","source":"class PetitPrinceDistribution:\n    def __init__(self, mu1=4.0, sigma1=1.5, mu2=0.6, sigma2=1.35):\n        self.mu1 = torch.tensor([mu1])\n        self.sigma1 = torch.tensor([sigma1])\n        self.mu2 = torch.tensor([mu2])\n        self.sigma2 = torch.tensor([sigma2])\n\n    def sample(self, N, seed=42):\n        # Set the seed for reproducibility:\n        torch.manual_seed(seed)\n        # Define the distribution:\n        m1 = torch.distributions.normal.Normal(loc=self.mu1, scale=self.sigma1)\n        m2 = torch.distributions.normal.Normal(loc=self.mu2, scale=self.sigma2)\n        samples = torch.cat((m1.sample([N//2]), m2.sample([N-N//2])), 0)\n        return samples","metadata":{"id":"_MewspOins62","execution":{"iopub.status.busy":"2022-04-22T21:58:17.069868Z","iopub.execute_input":"2022-04-22T21:58:17.070454Z","iopub.status.idle":"2022-04-22T21:58:17.078105Z","shell.execute_reply.started":"2022-04-22T21:58:17.070413Z","shell.execute_reply":"2022-04-22T21:58:17.077267Z"},"trusted":true},"execution_count":143,"outputs":[]},{"cell_type":"code","source":"data = PetitPrinceDistribution().sample(N=10000)","metadata":{"id":"6C_92GHQoMIS","execution":{"iopub.status.busy":"2022-04-22T21:58:17.185058Z","iopub.execute_input":"2022-04-22T21:58:17.185252Z","iopub.status.idle":"2022-04-22T21:58:17.190566Z","shell.execute_reply.started":"2022-04-22T21:58:17.185228Z","shell.execute_reply":"2022-04-22T21:58:17.189889Z"},"trusted":true},"execution_count":144,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nsns.distplot(data, norm_hist=True, rug=True, bins=50,\n             kde_kws={'color': 'k', 'linewidth': 3},\n             hist_kws={'color': 'chocolate'},\n             rug_kws={'alpha': 0.05, 'color': 'k'})\nplt.show()","metadata":{"id":"kE02dCAZpQut","outputId":"7fe9803c-b2a4-42f0-af65-58a859b856d7","execution":{"iopub.status.busy":"2022-04-22T21:58:17.341547Z","iopub.execute_input":"2022-04-22T21:58:17.341839Z","iopub.status.idle":"2022-04-22T21:58:17.818385Z","shell.execute_reply.started":"2022-04-22T21:58:17.341810Z","shell.execute_reply":"2022-04-22T21:58:17.817724Z"},"trusted":true},"execution_count":145,"outputs":[]},{"cell_type":"markdown","source":"# Exercise 2","metadata":{}},{"cell_type":"code","source":"\"\"\"\nTODO: Exercise 2, select your data distribution, define your Generator, Discriminator,\nparameters and hyperparameters. Show the GIF of your training as above, so make \nsure to change the root where you store these images when using `plot_distribution`\n\"\"\"","metadata":{"id":"LYYVPzaywMsU","outputId":"dc31c4f3-7058-4abd-b354-281682a3ec59","execution":{"iopub.status.busy":"2022-04-22T21:58:17.819920Z","iopub.execute_input":"2022-04-22T21:58:17.820175Z","iopub.status.idle":"2022-04-22T21:58:17.828373Z","shell.execute_reply.started":"2022-04-22T21:58:17.820141Z","shell.execute_reply":"2022-04-22T21:58:17.827647Z"},"trusted":true},"execution_count":146,"outputs":[]},{"cell_type":"code","source":"data = PetitPrinceDistribution().sample(N=10000)\ndata = LaplaceDistribution(loc=3.0, scale=0.3).sample(10000)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T22:06:46.954571Z","iopub.execute_input":"2022-04-22T22:06:46.954841Z","iopub.status.idle":"2022-04-22T22:06:46.962009Z","shell.execute_reply.started":"2022-04-22T22:06:46.954810Z","shell.execute_reply":"2022-04-22T22:06:46.960917Z"},"trusted":true},"execution_count":159,"outputs":[]},{"cell_type":"code","source":"generator = Generator(hidden_dim=64).to(device)\ngenerator.apply(weights_init)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T22:12:55.028239Z","iopub.execute_input":"2022-04-22T22:12:55.028487Z","iopub.status.idle":"2022-04-22T22:12:55.036787Z","shell.execute_reply.started":"2022-04-22T22:12:55.028459Z","shell.execute_reply":"2022-04-22T22:12:55.036060Z"},"trusted":true},"execution_count":195,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, input_size: int = 1, hidden_dim: int = 25):\n        super(Discriminator, self).__init__()\n\n        self.main = nn.Sequential(\n            # Input is one-dimensional\n            nn.Linear(in_features=input_size, out_features=hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(in_features=hidden_dim, out_features=hidden_dim),\n            nn.ReLU(inplace=True),\n            # Our output is a probability, so we use 1 output neuron with sigmoid:\n            nn.Linear(in_features=hidden_dim, out_features=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.main(x)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T22:15:37.025764Z","iopub.execute_input":"2022-04-22T22:15:37.026170Z","iopub.status.idle":"2022-04-22T22:15:37.034276Z","shell.execute_reply.started":"2022-04-22T22:15:37.026131Z","shell.execute_reply":"2022-04-22T22:15:37.032523Z"},"trusted":true},"execution_count":203,"outputs":[]},{"cell_type":"code","source":"discriminator = Discriminator(hidden_dim=64).to(device)\ndiscriminator.apply(weights_init)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T22:15:42.829984Z","iopub.execute_input":"2022-04-22T22:15:42.830730Z","iopub.status.idle":"2022-04-22T22:15:42.839932Z","shell.execute_reply.started":"2022-04-22T22:15:42.830689Z","shell.execute_reply":"2022-04-22T22:15:42.839048Z"},"trusted":true},"execution_count":204,"outputs":[]},{"cell_type":"code","source":"def train(num_epochs: int = 50,\n          lr: float = 1e-4,\n          num_eval: int = 1,\n          data: torch.Tensor = data,\n          hist: bool = True,\n          kde: bool = False,\n          d_repeats: int = 1,\n          root: Union[str, os.PathLike] = os.path.join(os.getcwd(), 'animation')):\n    # Some sanity check:\n    wow = \"Epochs must be at least 1 and an int!\"\n    assert num_epochs > 0 and isinstance(num_epochs, int), wow\n\n    # We will use Adam for both optimizers with same learning rate:\n    optimizerD = torch.optim.Adam(discriminator.parameters(), lr=lr)\n    optimizerG = torch.optim.Adam(generator.parameters(), lr=lr)\n\n    # We will keep track of the generator and discriminator losses:\n    G_losses = []\n    D_losses = []\n\n    # We will log the generated fake data (with the fixed_latent) in order to \n    # track our progression throughout training (i.e., plot it later on):\n    fake_data = []\n\n    # This will remove the previous run subdir (should this be the second time \n    # you run this code; make sure to save the previous run if you so wish):\n    shutil.rmtree(root, ignore_errors=True)\n\n    # Mark the beginning of training time:\n    start_time = time.time()\n\n    print(\"***** Starting training *****\")\n    for epoch in range(num_epochs):\n        # For each batch in our dataloader:\n        for i, d in enumerate(dataloader):\n            ############################################\n            #                  a.              b.\n            # Update D: max log(D(x)) + log(1 - D(G(z)))\n            ############################################\n            ### a. Train D with a real batch. First zero_grad the optimizer:\n            optimizerD.zero_grad()\n            # Move the batch data to the device:\n            real_b = d.to(device)\n            # We want the batch size to create the labels and latent vectors\n            # later on (remember the last batch won't be of size 1024):\n            b_size = real_b.size(0)\n            # Label of real data:\n            label = torch.full((b_size, ), \n                               real_label, # we fill it with ones\n                               device=device)\n            # Classify the real batch with D:\n            output = discriminator(real_b).view(-1)\n            # Calculate the loss on this real batch:\n            err_D_real = criterion(output, label)\n            # Calculate the gradients for D in backward pass:\n            err_D_real.backward()\n            # We calculate the average classification of the real data to monitor\n            # its progression (should start high, then get lower)\n            D_X = output.mean().item()\n\n            ### b. Train D with a fake batch:\n            # Generate a batch of latent vectors:\n            latents = get_latents(N=b_size, \n                                  latent_dim=latent_dim)\n            # Generate fake data with G:\n            fake_b = generator(latents).to(device)\n            # Let's reuse label (hence in-place fill) by filling with 0's:\n            label.fill_(fake_label)\n            # Classify the fake batch with D:\n            output = discriminator(fake_b.detach()).view(-1)\n            # Calculate D's loss on this fake batch:\n            err_D_fake = criterion(output, label)\n            # Calculate the gradients for D in backward pass:\n            err_D_fake.backward()\n            # We calculate the average classification of the fake data to monitor\n            # its progression:\n            D_G_z = output.mean().item()\n\n            # Add both gradients from the all-real and all-fake batches:\n            err_D = err_D_real + err_D_fake\n            # Once we've accumulated both gradients in the backward pass, we take\n            # a step:\n            optimizerD.step()\n\n            # Now on to train the Generator: remember we are in essence training\n            # D d_repeats every time we train G one time\n            \n            if epoch > 15:\n                d_repeats = 1\n            \n                \n            if i % d_repeats == 0:\n                ##############################\n                #                   c.\n                # Update G: max log(D(G(z)))\n                ##############################\n                ## c. Train G with a fake batch:\n                optimizerG.zero_grad()\n                # This time, for the Generator, we fill the label with 1's:\n                label.fill_(real_label)\n                # We updated D before this, so we make another forward-pass of an \n                # all-fake batch:\n                output = discriminator(fake_b).view(-1)\n                # Calculate G's loss based on this output:\n                err_G = criterion(output, label)\n                # Calculate the gradient for G:\n                err_G.backward()\n                # Update G:\n                optimizerG.step()\n\n            #######################################################################\n            # That's it for Algorithm 2; now on to printing some summary statistics\n            #######################################################################\n\n            # I am an order maniac, so I wish to print the number of necessary\n            # spaces depending on the number of digits for epochs and batches:\n            epoch_digits = int(np.log10(num_epochs)) + 1\n            batch_digits = int(np.log10(len(dataloader))) + 1\n            # Print our training statistics every num_eval:\n            if i % num_eval == 0:\n                # Get the current time:\n                runtime = time.time() - start_time\n                # Our code will run in much less than 1 hour, so we really only\n                # need space for 6 strings for the runtime (hence %-6s):\n                log_console = (\"\\r[%{}d/%{}d][%{}d/%{}d]  Runtime: %-6s  Loss_D: %.4f\"\n                \"  Loss_G: %.4f  D(x): %.4f  D(G(z)): %.4f\").format(*2*[epoch_digits], *2*[batch_digits])\n                # Print it:\n                print(log_console % (epoch, num_epochs, i, len(dataloader), format_time(runtime), \n                                     err_D.item(), err_G.item(), D_X, D_G_z))\n\n        # After every epoch, we save the losses:\n        G_losses.append(err_G.item())\n        D_losses.append(err_D.item())\n\n        # Check how the generator is doing by saving G's output on the fixed_latent\n        with torch.no_grad():\n            fake_X = generator(fixed_latent).detach().cpu()\n        fake_data.append(fake_X)\n        # Let's plot this fake data distribution and save it:\n        plot_distribution(data, \n                          fake_X, \n                          epoch, \n                          hist=hist,\n                          kde=kde,\n                          root=root)\n    print(\"\\n***** Finished training *****\")    \n    \n    # Return both losses for the Generator and Discriminator, as well as the \n    # fake data generated with our fixed_latent\n    return G_losses, D_losses, fake_data\n","metadata":{"execution":{"iopub.status.busy":"2022-04-22T22:15:44.299717Z","iopub.execute_input":"2022-04-22T22:15:44.302540Z","iopub.status.idle":"2022-04-22T22:15:44.338889Z","shell.execute_reply.started":"2022-04-22T22:15:44.302489Z","shell.execute_reply":"2022-04-22T22:15:44.337971Z"},"trusted":true},"execution_count":205,"outputs":[]},{"cell_type":"code","source":"G_losses, D_losses, fake_data = train(num_epochs=50, \n                                      lr=1e-3, \n                                      num_eval=4, \n                                      data=data,\n                                      hist=False,\n                                      kde=True,\n                                      d_repeats=2)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T22:15:45.597094Z","iopub.execute_input":"2022-04-22T22:15:45.597952Z","iopub.status.idle":"2022-04-22T22:16:07.039053Z","shell.execute_reply.started":"2022-04-22T22:15:45.597908Z","shell.execute_reply":"2022-04-22T22:16:07.037508Z"},"trusted":true},"execution_count":206,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(8, 6))\nax = fig.add_axes([0, 0, 1, 1])\nplt.plot(D_losses, label=\"Discriminator loss\")\nplt.plot(G_losses, label=\"Generator loss\")\nax.axhline(np.log(2), c='orange', ls='--', label='$\\log{2}$')\nax.axhline(np.log(4), c='blue', ls='--', label='$\\log{4}$')\nplt.legend()\nplt.xlabel(\"Epoch\")\nplt.title('NEW SETUP')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T22:16:07.043168Z","iopub.execute_input":"2022-04-22T22:16:07.043380Z","iopub.status.idle":"2022-04-22T22:16:07.311649Z","shell.execute_reply.started":"2022-04-22T22:16:07.043354Z","shell.execute_reply":"2022-04-22T22:16:07.310847Z"},"trusted":true},"execution_count":207,"outputs":[]},{"cell_type":"code","source":"means = [d.mean() for d in fake_data]\nstds = [d.std() for d in fake_data]\n\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_axes([0, 0, 1, 1])\nplt.plot(means, label='Generated mean')\nax.axhline(y=actual_mean, c='blue', ls='--', label=f'Actual mean: {actual_mean:.3f}')\nplt.plot(stds, label='Generated standard dev')\nax.axhline(y=actual_std, c='orange', ls='--', label=f'Actual std dev: {actual_std:.3f}')\nplt.xlabel(\"Epoch\")\nplt.title(f\"Final mean: {means[-1]:.3f}, Final std dev: {stds[-1]:.3f}\")\nplt.legend()\nplt.title('NEW SETUP')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T22:16:07.313178Z","iopub.execute_input":"2022-04-22T22:16:07.313438Z","iopub.status.idle":"2022-04-22T22:16:07.557460Z","shell.execute_reply.started":"2022-04-22T22:16:07.313401Z","shell.execute_reply":"2022-04-22T22:16:07.556523Z"},"trusted":true},"execution_count":208,"outputs":[]},{"cell_type":"markdown","source":"The model after 50 epochs can approximate the real mean and std of the real distribution","metadata":{}},{"cell_type":"markdown","source":"# Further Resources\n\n* [*GAN*](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf), I. Goodfellow-et-al, 2014.\n    * [*Google's GAN Course*](https://developers.google.com/machine-learning/gan)\n    * [*Coursera's GAN Specialization*](https://www.coursera.org/specializations/generative-adversarial-networks-gans), S. Zhou, E. Zhou, E. Zelikman\n    * [*Deep Learning*](https://www.deeplearningbook.org), Goodfellow-et-al, 2016, MIT Press (specifically, Ch. 20, [*Deep Generative Models*](https://www.deeplearningbook.org/contents/generative_models.html), pp. 696-699)\n    * [*Improved Techniques for Training GANs*](https://arxiv.org/abs/1606.03498), T. Salimans-et-al, 2016\n    * [*NIPs 2016 Tutorial: GANs*](https://arxiv.org/abs/1701.00160), I. Goodfellow, NIPs 2016; check the [recorded video](https://youtu.be/HGYYEUSm-0Q) of the event as well\n    * [*GAN Lab*](https://poloclub.github.io/ganlab/), M. Kahng-et-al, 2018. 2D GAN that can be trained on the browser (using [TensorFlow.js](https://www.tensorflow.org/js/)).\n* [*DCGAN*](https://arxiv.org/abs/1511.06434), A. Radford-et-al, 2015.\n    * [*GAN Hacks*](https://github.com/soumith/ganhacks), S. Chintala-et-al, 2016.\n    * [*DCGAN Tensorflow tutorial*](https://www.tensorflow.org/tutorials/generative/dcgan)\n    * [*Deconvolution and Checkerboard Artifacts*](https://distill.pub/2016/deconv-checkerboard/), A.Odena-et-al, 2016 (particularly important for DCGAN and beyond).\n* [*WGAN*](https://arxiv.org/abs/1701.07875), M. Arjovsky-et-al, 2017.\n    * [*From GAN to WGAN*](https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html), L. Weng, 2017. \n* [*StyleGAN*](https://arxiv.org/abs/1812.04948)\n    * [*Official `TensorFlow` implementation*](https://github.com/NVlabs/stylegan)\n    * https://thisvesseldoesnotexist.com\n    * [*Making Anime Faces with StyleGAN*](https://www.gwern.net/Faces), G. Branwen, 2019 (fun blog, but especially useful if you wish to start with SOTA GANs and their training specifics).\n* [*StyleGAN2*](https://arxiv.org/abs/1912.04958)\n    * [*Official `TensorFlow` implementation*](https://github.com/NVlabs/stylegan2)\n    * [*Colab notebook*](https://colab.research.google.com/drive/1ShgW6wohEFQtqs_znMna3dzrcVoABKIH)\n    * Works best if the dataset is aligned (e.g., for faces: eyes, mouth, nose in the same position...)\n    * https://www.thispersondoesnotexist.com/\n    * https://www.thiswaifudoesnotexist.net/\n* [*StyleGAN2-ADA*](https://arxiv.org/abs/2006.06676)\n    * [Official `PyTorch` implementation](https://github.com/NVlabs/stylegan2-ada-pytorch) (a bit faster to train vs. the `TensorFlow` one)\n    * Meant to be used for small datasets, e.g., on the order of 10^3\n* [*StyleGAN3*](https://arxiv.org/abs/2106.12423)\n    * [Official `PyTorch` implementation](https://github.com/NVlabs/stylegan3)\n    * Meant as a step towards generating animation, as well as removing the need to align the datasets (with the configs having translational and rotational equivariances)\n* [BigGAN](https://arxiv.org/abs/1809.11096)\n    * [*Colab notebook*](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb) (impossible to train your own from scratch, so only pretrained models exist)\n    * [*Art Breeder*](https://artbreeder.com/), J. Simon, 2018.\n* **Beyond...**\n    * [*Are GANs Created Equal? A Large-Scale Study*](https://arxiv.org/abs/1711.10337), M. Lucic-et-al, 2017.\n    * [*Generative Models*](https://github.com/wiseodd/generative-models), A. Kristiadi, 2018. A collection of many generative models (GANs and VAEs included), coded in `PyTorch` and `TensorFlow`.\n    * Make your own [<del>1D</del> *2D GAN*](https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-a-1-dimensional-function-from-scratch-in-keras/)! I digress with the author on the name of the project: while the function is indeed one-dimensional, the data you are feeding to the GAN is in fact two-dimensional (points in 2D space), hence my renaming.\n    * [*Open Questions About GANs*](https://distill.pub/2019/gan-open-problems/), A. Odena, 2019.\n    * [*A Review on GANs: Algorithms, Theory and Applications*](https://arxiv.org/abs/2001.06937), J. Gui-et-al, 2020. This is the paper you must read if you want a quick overview of the last ~6 years of GAN history.","metadata":{"id":"4p-UZe_15J10"}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}